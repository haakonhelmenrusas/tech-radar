name,ring,quadrant,isNew,status,description
Continuous compliance,Adopt,Techniques,FALSE,moved in,"<p><strong>Continuous compliance</strong> is the practice of ensuring that software development processes and technologies meet regulatory and security standards on an ongoing basis through automation. Manual compliance checks can slow development and introduce human error, whereas automated checks and audits provide faster feedback, clearer evidence and simplified reporting. </p>
<p>By integrating <a href=""/radar/techniques/security-policy-as-code"">policy-as-code</a> tools such as <a href=""/radar/tools/open-policy-agent-opa"">Open Policy Agent</a> and generating <a href=""/radar/techniques/software-bill-of-materials"">SBOMs</a> within CD pipelines — aligned with <a href=""/radar/techniques/slsa"">SLSA</a> guidance — teams can detect and address compliance issues early. Codifying rules and best practices enforces standards consistently across teams without creating bottlenecks. <a href=""/radar/languages/open-security-control-assessment-language-oscal"">OSCAL</a> also shows promise as a framework for automating compliance at scale.</p>
<p>Practices and tooling for continuous compliance are now mature enough that it should be treated as a sensible default, which is why we’ve moved our recommendation to Adopt. The increasing use of AI in coding — and the accompanying risk of <a href=""/radar/techniques/complacency-with-ai-generated-code"">complacency with AI-generated code</a> — makes embedding compliance into the development process more critical than ever.</p>"
Curated shared instructions for software teams,Adopt,Techniques,TRUE,new,"<p>For teams actively using AI in software delivery, the next step is moving beyond individual prompting toward <strong>curated instructions for software teams</strong>. This practice helps you apply AI effectively across all delivery tasks — not just coding — by sharing proven, high-quality instructions. The most straightforward way to implement this is by committing instruction files, such as an <a href=""/radar/techniques/agents-md"">AGENTS.md</a>, directly to your project repository. Most AI-coding tools — including <a href=""/radar/tools/cursor"">Cursor</a>, <a href=""/radar/tools/windsurf"">Windsurf</a> and <a href=""/radar/tools/claude-code"">Claude Code</a> — support sharing instructions through custom slash commands or workflows. For noncoding tasks, you can set up organization-wide prompt libraries ready to use. This systematic approach allows for continuous improvement: As soon as a prompt is refined, the entire team benefits, ensuring consistent access to the best AI instructions.</p>"
Pre-commit hooks,Adopt,Techniques,TRUE,new,"<p><strong><a href=""https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks"">Git hooks</a></strong> have been around for a long time, but we feel they’re still underused. The rise of AI-assisted and agentic coding has increased the risk of accidentally committing secrets or problematic code. While there are many mechanisms for code validation, such as <a href=""https://martinfowler.com/articles/continuousIntegration.html"">Continuous Integration</a>, pre-commit hooks are a simple and effective safeguard that more teams should adopt. However, overloading hooks with slow-running checks can discourage developers from using them, so it's best to keep them minimal and focused on risks that are most effectively caught at this stage of the workflow, such as secret scanning.</p>"
Using GenAI to understand legacy codebases,Adopt,Techniques,FALSE,moved in,"<p>In recent months, we’ve seen clear evidence that <strong>using GenAI to understand legacy codebases</strong> can significantly accelerate comprehension of large and complex systems. Tools such as <a href=""/radar/tools/cursor"">Cursor</a>, <a href=""/radar/tools/claude-code"">Claude Code</a>, <a href=""https://github.com/features/copilot"">Copilot</a>, <a href=""https://windsurf.com/"">Windsurf</a>, <a href=""https://aider.chat/"">Aider</a>, <a href=""https://meetcody.ai/"">Cody</a>, <a href=""https://swimm.io/"">Swimm</a>, <a href=""https://getunblocked.com/"">Unblocked</a> and <a href=""https://github.com/The-Pocket/PocketFlow-Tutorial-Codebase-Knowledge"">PocketFlow-Tutorial-Codebase-Knowledge</a> help developers surface business rules, summarize logic and identify dependencies. Used alongside open frameworks and direct LLM prompting, they dramatically reduce the time needed to understand legacy codebases.</p>
<p>Our experience across multiple clients shows that GenAI-assisted understanding of legacy systems is now a practical default rather than an experiment. Setup effort varies, particularly for advanced approaches such as <a href=""/radar/techniques/graphrag"">GraphRAG</a>, and tends to scale with the size and complexity of the codebase being analyzed. Despite this, the impact on productivity gains are consistent and substantial. GenAI has become an essential part of how we explore and understand legacy systems.</p>"
AGENTS.md,Trial,Techniques,TRUE,new,"<p><strong><a href=""https://agents.md/"">AGENTS.md</a></strong> is a common format for providing instructions to AI coding agents working on a project. Essentially a README file for agents, it has no required fields or formatting other than Markdown, relying on the ability of LLM-based coding agents to interpret human-written, human-readable guidance. Typical uses include tips on using tools in the coding environment, testing instructions and preferred practices for managing commits. While AI tools support various methods for <a href=""/radar/techniques/context-engineering"">context engineering</a>, the value of AGENTS.md lies in creating a simple convention for a file that acts as a starting point.</p>"
AI for code migrations,Trial,Techniques,TRUE,new,"<p>Code migrations take many forms — from language rewrites to dependency or framework upgrades — and are rarely trivial, often requiring months of manual effort. One of our teams, when <a href=""https://www.thoughtworks.com/insights/blog/generative-ai/automating-framework-upgrades-can-combination-of-AI-and-traditional-tooling-help"">upgrading their .NET framework version</a>, experimented with using AI to shorten the process. In the past, we blipped <a href=""/radar/tools/openrewrite"">OpenRewrite</a>, a deterministic, rule-based refactoring tool. Using AI alone for such upgrades has often proven costly and prone to meandering conversations. Instead, the team combined traditional upgrade pipelines with agentic coding assistants to manage complex transitions. Rather than delegating a full upgrade, they broke the process into smaller, verifiable steps: analyzing compilation errors, generating migration diffs and validating tests iteratively. This hybrid approach positions AI coding agents as pragmatic collaborators in software maintenance. Industry examples, such as <a href=""https://www.theregister.com/2025/01/16/google_ai_code_migration/"">Google’s large-scale int32-to-int64 migration</a>, reflect a similar trend. While our results are mixed in measurable time savings, the potential to reduce developer toil is clear and worth continued exploration.</p>"
Delta Lake Liquid Clustering,Trial,Techniques,TRUE,new,"<p><strong><a href=""https://delta.io/blog/liquid-clustering/"">Liquid Clustering</a></strong> is a technique for <a href=""/radar/platforms/delta-lake"">Delta Lake</a> tables that serves as an alternative to partitioning and Z-ordering. Historically, optimizing Delta tables for read performance required defining partition and Z-order keys at table creation based on anticipated query patterns. Modifying these keys later necessitates a full data rewrite. In contrast, Liquid Clustering employs a tree-based algorithm to cluster data based on designated keys, which can be incrementally changed without rewriting all data. This provides greater flexibility to support diverse query patterns, thereby reducing compute costs and enhancing read performance. Furthermore, the Databricks Runtime for Delta Lake supports <a href=""https://docs.databricks.com/aws/en/delta/clustering#automatic-liquid-clustering"">Automatic Liquid Clustering</a> by analyzing historical query workloads, identifying optimal columns, and clustering data accordingly. Both standalone Delta Lake and Databricks Runtime users can leverage Liquid Clustering to optimize read performance.</p>"
Self-serve UI prototyping with GenAI,Trial,Techniques,TRUE,new,"<p>We use the phrase <strong>self-serve UI prototyping with GenAI</strong> to describe an emerging technique where tools such as Claude Code, <a href=""https://www.figma.com/make/"">Figma Make</a>, <a href=""https://miro.com/ai"">Miro AI</a> and <a href=""/radar/tools/v0"">v0</a> enable product managers to generate interactive, user-testable prototypes directly from text prompts. Instead of manually crafting wireframes, teams can generate functional HTML, CSS and JS artifacts in minutes — offering the speed of a sketch, but with real interactivity and higher fidelity. These “throwaway” prototypes trade polish for rapid learning, making them ideal for early validation during design sprints. However, higher fidelity can lead to misplaced focus on small details or unrealistic expectations about production effort — making clear framing and expectation management essential.</p>
<p>Used alongside user research, this technique accelerates discovery by turning abstract ideas into tangible experiences for users to react to. However, teams should take care not to let these tools replace the research process itself. Done well, self-serve prototyping shortens feedback loops, lowers barriers for non-designers and helps teams iterate quickly while maintaining a healthy balance between speed and quality.</p>"
Structured output from LLMs,Trial,Techniques,FALSE,moved in,"<p><strong>Structured output from LLMs</strong> is the practice of constraining a large language model to produce responses in a predefined format — such as JSON or a specific programming class. This technique is essential for building reliable, production-grade applications, transforming the LLM's typically unpredictable text into a machine-readable, deterministic data contract. Based on successful production use, we’re moving this technique from Assess to Trial.</p>
<p>Approaches range from simple prompt-based formatting and <a href=""https://platform.openai.com/docs/guides/structured-outputs"">model-native structured outputs</a> to more robust constrained decoding methods using tools like <a href=""https://github.com/dottxt-ai/outlines"">Outlines</a> and <a href=""https://github.com/567-labs/instructor"">Instructor</a>, which apply finite-state machines to guarantee valid output. We’ve successfully used this technique to extract complex, unstructured data from diverse document types and convert it into structured JSON for downstream business logic.</p>"
TCR (Test && Commit || Revert),Trial,Techniques,TRUE,new,"<p><strong>Test &amp;&amp; commit || revert</strong> (TCR) is a programming workflow derived from test-driven development that promotes very small, continuous steps through a simple rule: After each change, if the tests pass, the changes are committed; if they fail, the changes are reverted. Implementing TCR is straightforward: You only need to define a script that automates this cycle within your codebase. Originally introduced in a canonical <a href=""https://medium.com/@kentbeck_7670/test-commit-revert-870bbd756864"">article</a> by Kent Beck, we've found that TCR reinforces positive coding practices such as <a href=""https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it"">YAGNI</a> and <a href=""https://en.wikipedia.org/wiki/KISS_principle"">KISS</a>. It's worth evaluating as we experiment with new workflows for building software using GenAI.</p>"
AI-powered UI testing,Assess,Techniques,FALSE,no change,"<p>In the previous Radar, <strong>AI-powered UI testing</strong> primarily focused on exploratory testing, where we noted that the non-determinism of LLMs could introduce flakiness. With the rise of <a href=""/radar/platforms/model-context-protocol-mcp"">MCP</a>, we’re now seeing major UI testing frameworks like <a href=""/radar/languages-and-frameworks/playwright"">Playwright</a> and Selenium introduce their own MCP servers (<a href=""https://github.com/microsoft/playwright-mcp"">playwright-mcp</a>, <a href=""https://github.com/angiejones/mcp-selenium"">mcp-selenium</a>). These provide reliable browser automation through their native technologies, enabling coding assistants to generate reliable UI tests in Playwright or Selenium. While AI-powered UI testing remains a fast-evolving space — the latest Playwright release, for example, introduced <a href=""https://playwright.dev/docs/test-agents"">Playwright Agents</a> — we’re excited about those developments and look forward to seeing more practical guidance and field experience emerge.</p>"
Anchoring coding agents to a reference application,Assess,Techniques,TRUE,new,"<p>In the past we blipped the <a href=""/radar/techniques/tailored-service-templates"">Tailored Service Templates</a> pattern, which helped organizations adopting microservices by providing sensible defaults to bootstrap new services and integrate them seamlessly with existing infrastructure. Over time, however, code drift between these templates and existing services tends to grow as new dependencies, frameworks and architectural patterns emerge. To maintain good practices and architectural consistency — especially in the age of <a href=""/radar/techniques/team-of-coding-agents"">coding agents</a> — we’ve been experimenting with <a href=""https://martinfowler.com/articles/exploring-gen-ai/anchoring-to-reference.html"">anchoring coding agents to a reference application</a>. This pattern guides generative code agents by providing a live, compilable reference application instead of static prompt examples. A <a href=""/radar/platforms/model-context-protocol-mcp"">Model Context Protocol (MCP)</a> server exposes both reference template code and commit diffs, enabling agents to detect drift and propose repairs. This approach transforms static templates into living, adaptable blueprints that AI can reference intelligently — maintaining consistency, reducing divergence and improving control over AI-driven scaffolding as systems evolve.</p>"
Context engineering,Assess,Techniques,TRUE,new,"<p><strong>Context engineering</strong> is the systematic design and optimization of the information provided to a large language model during inference to reliably produce the desired output. It involves structuring, selecting and sequencing contextual elements — such as prompts, retrieved data, memory, instructions and environmental signals — so the model’s internal layers operate in an optimal state. Unlike prompt engineering, which focuses only on the wording of prompts, context engineering considers the entire configuration of context: how relevant knowledge, instructions and prior context are organized and delivered to achieve the most effective results.</p>
<p>Today, engineers use a range of discrete techniques that can be grouped into three areas: <em>Context setup</em> covers curation tactics such as using minimal <a href=""https://www.promptingguide.ai/introduction/tips"">system prompts</a>, canonical <a href=""https://www.promptingguide.ai/techniques/fewshot"">few-shot examples</a> and token-efficient <a href=""https://www.anthropic.com/engineering/writing-tools-for-agents"">tools</a> for decisive action. <em>Context management for long-horizon tasks</em> addresses finite context windows through <a href=""https://blog.langchain.com/context-engineering-for-agents/"">context summarization</a> , <a href=""https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents"">structured note-taking</a> to persist external memories and <a href=""https://docs.claude.com/en/docs/claude-code/sub-agents"">sub-agent architectures</a> to isolate and summarize complex sub-tasks. <em>Dynamic information retrieval</em> relies on <a href=""https://jentic.com/blog/just-in-time-tooling"">just-in-time (JIT) context retrieval</a>, where agents autonomously load external data only when immediately relevant, maximizing efficiency and precision.</p>"
GenAI for forward engineering,Assess,Techniques,TRUE,new,"<p><strong>GenAI for forward engineering</strong> is an emerging technique for modernizing legacy systems through AI-generated descriptions of legacy codebases. It introduces an explicit step focused on <em>what</em> the legacy code does (its specification) while deliberately hiding <em>how</em> it’s currently implemented. This relates to <a href=""radar/techniques/spec-driven-development"">spec-driven development</a> but is specifically applied to legacy modernization. </p>
<p>By generating and iterating on functional descriptions before rewriting the code, teams can use GenAI to surface hidden logic, dependencies and edge cases that might otherwise be overlooked. Emphasizing the problem space rather than the existing system also allows GenAI models to explore more creative and forward-looking solutions. The workflow follows a reverse-engineering → design/solutioning → forward-engineering loop, which enables both humans and AI agents to reason at a higher level before committing to an implementation.</p>
<p>At Thoughtworks, we’re seeing multiple teams apply this approach successfully to accelerate the rewrite of legacy systems. The goal isn’t to obscure implementation details entirely, but to introduce a temporary abstraction that helps teams and agents explore alternatives without being constrained by the current structure. This technique is showing promising results in producing cleaner, more maintainable and future-ready code while also reducing the time spent understanding existing implementations.</p>"
GraphQL as data access pattern for LLMs,Assess,Techniques,TRUE,new,"<p><strong>GraphQL as a data access pattern for LLMs</strong> is an emerging approach for creating a uniform, model-friendly data access layer that enhances context engineering. It enables teams to expose structured, queryable data without granting models direct database access. Unlike REST APIs, which tend to over-fetch data or require new endpoints or filters for each use case, <a href=""/radar/languages-and-frameworks/graphql"">GraphQL</a> lets the model retrieve only the data it needs — reducing noise, improving context relevance and cutting token usage.</p>
<p>A well-defined GraphQL schema also provides metadata that LLMs can use to reason about available entities and relationships, enabling dynamic, schema-aware querying for agentic use cases. This pattern offers a secure middle ground between REST and SQL, balancing governance controls with flexible access.</p>
<p>However, the approach relies on well-structured schemas and meaningful field names. Interpreting schema semantics and navigating complex structures remain challenging — and what is difficult for people to reason about is often equally difficult for LLMs. It’s also important to be cognizant of the increased vectors for DoS attacks as well as typical GraphQL challenges, such as caching and versioning.</p>"
Knowledge Flows over Knowledge Stocks,Assess,Techniques,TRUE,new,"<p>One question we get asked a lot is ""How do we improve the way we share information amongst our teams?"" Techniques for managing organizational knowledge continue to evolve, and one perspective we've found valuable borrows from systems thinking: the concepts of <strong>knowledge flows and knowledge stocks</strong>. Originally from economics, this framing encourages teams to view their organizational knowledge as a system — stocks representing accumulated knowledge and flows representing how knowledge moves and evolves through the organisation. Increasing the flow of external knowledge into an organization tends to <a href=""https://doi.org/10.1016/j.respol.2015.03.003"">boost innovation</a>. A time-tested way to improve flow is to establish communities of practice, which consistently show <a href=""https://cacm.acm.org/research/spotify-guilds/"">measurable benefits</a>. Another is by deliberately seeking diverse, external sources of insight. As GenAI tools make existing knowledge stocks more accessible, it's worth remembering that nurturing fresh ideas and external perspectives is just as critical as adopting new technologies.</p>"
LLM as a judge,Assess,Techniques,FALSE,no change,"<p>Using an <strong>LLM as a judge</strong> — to evaluate the output of another system, usually an LLM-based generator — has garnered attention for its potential to deliver scalable, automated evaluation in generative AI. However, we’re moving this blip from Trial to Assess to reflect newly recognized complexities and risks.</p>
<p>While this technique offers speed and scale, it often fails as a reliable proxy for human judgment. Evaluations are prone to position bias, verbosity bias and low robustness. A more serious issue is <a href=""https://arxiv.org/abs/2502.01534"">scaling contamination</a>: When LLM as a Judge is used in training pipelines for reward modeling, it can introduce self-enhancement bias — where a model family favors its own outputs — and preference leakage, blurring the boundary between training and testing. These flaws have led to overfitted results that inflate performance metrics without real-world validity. There have been <a href=""https://arxiv.org/abs/2508.18076"">research studies</a> that conduct more rigorous investigations into this pattern. To counter these flaws, we are exploring improved techniques, such as using <a href=""https://arxiv.org/abs/2404.18796"">LLMs as a jury</a> (employing multiple models for consensus) or chain-of-thought reasoning during evaluation. While these methods aim to increase reliability, they also increase cost and complexity. We advise teams to treat this technique with caution — ensuring human verification, transparency, and ethical oversight before incorporating LLM judges into critical workflows. The approach remains powerful but less mature than once believed.</p>"
On Device Information Retrieval,Assess,Techniques,TRUE,new,"<p><strong>On-device information retrieval</strong> is a technique that enables search, context-awareness and <a href=""/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> to run entirely on user devices — mobile, desktop or edge devices — prioritizing privacy and computational efficiency. It combines a lightweight local database with a model optimized for on-device inference. A promising implementation pairs <a href=""https://github.com/asg017/sqlite-vec"">sqlite-vec</a>, a SQLite extension that supports vector search within the embedded database, with <a href=""https://developers.googleblog.com/en/introducing-embeddinggemma/"">EmbeddingGemma</a>, a 300 million parameter embedding model built on the <a href=""https://developers.googleblog.com/en/gemma-explained-embeddinggemma-architecture-and-recipe/"">Gemma 3 architecture</a>. Optimized for efficiency and resource-constrained environments, this combination keeps data close to the edge, reducing dependence on cloud APIs and improving latency and privacy. We recommend teams assess this technique for <a href=""/radar/techniques/local-first-application"">local-first applications</a> and other use cases where data sovereignty, low-latency and privacy are critical.</p>"
SAIF,Assess,Techniques,TRUE,new,"<p><strong><a href=""https://saif.google/"">SAIF</a></strong> (Secure AI Framework) is a framework developed by Google to provide a practical guide for managing AI security risks. It systematically addresses common threats such as data poisoning and prompt injection through a clear risk map, component analysis. SAIF also provides practical mitigation strategies for each threats. We find its focus on the evolving risks of building agentic systems especially timely and valuable. SAIF offers a concise, actionable playbook that teams can use to strengthen security practices for LLM usage and AI-driven applications.</p>"
Service mesh without sidecar,Assess,Techniques,FALSE,no change,"<p>As the cost and operational complexity of sidecar-based service meshes persists, we’re excited to see another option for <strong>service mesh without sidecar</strong> emerge: <a href=""https://istio.io/latest/docs/ambient/"">Istio ambient mode</a>. Ambient mode introduces a layered architecture that separates concerns between two key components: the per-node L4 proxy (ztunnel) and the per-namespace L7 proxy (Waypoint proxy). ztunnel ensures that L3 and L4 traffic is transported efficiently and securely. It powers the ambient data plane by fetching certificates for all node identities and handles traffic redirection to and from ambient-enabled workloads. The Waypoints proxy, an optional ambient mode component, enables richer Istio features such as traffic management, security and observability. We’ve had good experience with ambient mode in small-scale clusters and look forward to gaining more large-scale insights and best practices as adoption grows.</p>"
Small language models,Assess,Techniques,FALSE,no change,"<p>We’ve observed steady progress in the development of <strong>small language models </strong>(SLMs) across several volumes of the Technology Radar. With growing interest in building agentic solutions, we’re seeing increasing evidence that SLMs can power agentic AI efficiently. Most current agentic workflows are focused on narrow, repetitive tasks that don’t require advanced reasoning, making them a good match for SLMs. Continued advancements in SLMs such as Phi-3, SmolLM2 and DeepSeek suggest that SLMs offer sufficient capability for these tasks — with the added benefits of lower cost, reduced latency and lower resource consumption compared to LLMs. It’s worth considering SLMs as the default choice for agentic workflows, reserving larger, more resource-intensive LLMs only when necessary.</p>"
Spec-Driven Development,Assess,Techniques,TRUE,new,"<p><strong>Spec-driven development</strong> is an emerging approach to AI-assisted coding workflows. While the term's definition is still evolving, it generally refers to workflows that begin with a structured functional specification, then proceed through multiple steps to break it down into smaller pieces, solutions and tasks. The specification can take many forms: a single document, a set of documents or structured artifacts that capture different functional aspects.</p>
<p>We’ve seen many developers adopt this style (and have one of our own that we’re sharing internally at Thoughtworks). Three tools in particular have recently explored distinct interpretations of spec-driven development. Amazon's <a href=""https://kiro.dev/"">Kiro</a> guides users through three workflow stages — requirements, design and tasks creation. <a href=""https://github.com/github/spec-kit"">GitHub's spec-kit</a> follows a similar three-step process but adds richer orchestration, configurable prompts and a “constitution” defining immutable principles that must always be followed. <a href=""https://tessl.io/"">Tessl Framework</a> (still in private beta as of September 2025) takes a more radical approach in which the specification itself becomes the maintained artifact, rather than the code. </p>
<p>We find this space fascinating, though the workflows remain elaborate and opinionated. These tools behave very differently depending on task size and type; some generate lengthy spec files that are hard to review, and when they produce PRDs or user stories, it's sometimes unclear who their intended user is. We may be relearning a <a href=""https://en.wikipedia.org/wiki/Bitter_lesson"">bitter lesson</a> — that handcrafting detailed rules for AI ultimately doesn’t scale.</p>"
Team of coding agents,Assess,Techniques,TRUE,new,"<p><strong>Team of coding agents</strong> refers to a technique in which a developer orchestrates multiple AI coding agents, each with a distinct role — for example, architect, back-end specialist, tester — to collaborate on a development task. This practice is supported by tools such as <a href=""/radar/tools/claude-code"">Claude Code</a>, <a href=""https://github.com/RooCodeInc/Roo-Code"">Roo Code</a> and <a href=""https://kilocode.ai/"">Kilo Code</a> which enable subagents and multiple operating modes. Building on the proven principle that assigning LLMs specific roles and personas improves output quality, the idea is to achieve better results by coordinating multiple role-specific agents rather than relying on a single general-purpose one. The optimal level of agent is still being explored, but it can extend beyond simple one-to-one mappings with traditional team roles. This approach marks a shift toward orchestrated, multi-step AI-assisted development pipelines.</p>"
Topology Aware Scheduling,Assess,Techniques,TRUE,new,"<p>GPUs and LPUs are no longer standalone devices but tightly coupled networks of accelerators whose performance depends on placement and topology. In rack-scale systems like NVIDIA’s NVL72, 72 GPUs share over 13 TB of VRAM and act as a single accelerator — until workloads cross-switch islands, turning collective operations into bottlenecks. Similarly, Groq’s compile-time, software-scheduled architecture assumes deterministic data movement; random scheduling breaks those assumptions and predictability. Even within the same datacenter, GPU performance can vary significantly, creating demand for <strong>topology-aware scheduling</strong> that accounts for both hardware layout and variability when placing jobs.</p>
<p>Naive schedulers that ignore NVLink, PCIe or NIC topology often scatter multi-GPU workloads arbitrarily, resulting in degraded step time and efficiency. Training workloads, which are synchronous and bandwidth-bound, favor contiguous NVLink islands with uniform, high-bandwidth paths for all-reduce and pipeline stages. These jobs should co-schedule based on fabric bandwidth, avoid cross-switch hops and treat link, switch and node boundaries as failure domains. Inference workloads, by contrast, are latency and SLO-bound and typically balance replication for high availability across domains with sharding to keep mixture of experts (MoE) and KV-cache locality on the shortest paths. Optimizing placement for prefill versus decode phases, micro-batching and tenant isolation further improves efficiency. We believe topology-aware scheduling will become essential as accelerator performance grows increasingly dependent on network and datacenter topology. Our teams are already assessing <a href=""/radar/tools/kueue"">Kueue</a> and related projects to improve placement precision, boost performance and ensure reliable scaling for our clients.</p>"
Toxic Flow Analysis for AI,Assess,Techniques,TRUE,new,"<p>The now-familiar joke that the S in MCP stands for “security” hides a very real problem. When agents communicate with one another — through tool invocation or API calls — they can quickly encounter what's become known as the <a href=""https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/"">lethal trifecta</a>: access to private data, exposure to untrusted content and the ability to communicate externally. Agents with all three are highly vulnerable. Because LLMs tend to follow instructions in their input, content that includes a directive to exfiltrate data to an untrusted source can easily lead to data leaks. One emerging technique to mitigate this risk is <strong><a href=""https://invariantlabs.ai/blog/toxic-flow-analysis"">toxic flow analysis</a></strong>, which examines the flow graph of an agentic system to identify potentially unsafe data paths for further investigation. While still in its early stages, toxic flow analysis represents one of several promising approaches to detecting the new attack vectors that agentic systems and MCP servers are increasingly exposed to.</p>"
AI-accelerated shadow IT,Hold,Techniques,FALSE,no change,"<p>Sam's Gambit by EE</p>
<p>AI is lowering the barriers for noncoders to build and integrate software themselves, instead of waiting for the IT department to get around to their requirements. While we’re excited about the potential this unlocks, we’re also wary of the first signs of <strong>AI-accelerated shadow IT</strong>. No-code workflow automation platforms now support AI API integration (e.g., OpenAI or Anthropic), making it tempting to use AI as duct tape — stitching together integrations that previously weren’t possible, such as turning chat messages in one system into ERP API calls via AI. At the same time, AI coding assistants are becoming more agentic, enabling noncoders with basic training to build internal utility applications.</p>
<p>This has all the hallmarks of the next evolution of the spreadsheets that still power critical processes in some enterprises — but with a much bigger footprint. Left unchecked, this new shadow IT could lead to a proliferation of ungoverned, potentially insecure applications, scattering data across more and more systems. Organizations should be aware of these risks and carefully weigh the trade-offs between rapid problem-solving and long-term stability.</p>"
Capacity Driven Development,Hold,Techniques,TRUE,new,"<p>Key to the success of modern software development practices is maintaining a focus on the flow of work. Stream-aligned teams concentrate on a single, valuable flow — such as a user journey or product — which allows them to deliver end-to-end value efficiently. However, we’re seeing a concerning trend toward <strong>capacity-driven development</strong>, where teams aligned in this way take on features from other products or streams when they have spare capacity. While this may seem efficient in the short term, it's a local optimization best suited to handling sudden spikes in demand. When normalized it increases cognitive load and technical debt, and in the worst case can lead to congestion collapse as the cost of context switching across products compounds. Teams with <a href=""https://martinfowler.com/bliki/Slack.html"">spare capacity</a> are better served by focusing on improving <a href=""/radar/techniques/tracking-health-over-debt"">system health</a>. To manage capacity effectively, use WIP limits to control work across adjacent streams, consider cross-training to balance periods of high demand and apply <a href=""https://agilealliance.org/resources/experience-reports/dynamic-reteaming-how-we-thrive-by-rebuilding-teams/"">dynamic reteaming techniques</a> when needed.</p>"
Complacency with AI-generated code,Hold,Techniques,FALSE,no change,"<p>As AI coding assistants and agents gain traction, so does the body of data and research highlighting concerns about <strong>complacency with AI-generated code</strong>. While there’s ample evidence these tools can accelerate development — especially for prototyping and greenfield projects — studies show that code quality can decline over time.</p>
<p>GitClear's 2024 <a href=""https://www.gitclear.com/ai_assistant_code_quality_2025_research"">research</a> found that duplicate code and code churn have risen more than expected, while refactoring activity in commit histories has dropped. Reflecting a similar trend, <a href=""https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/"">Microsoft research</a> on knowledge workers shows that AI-driven confidence often comes at the expense of critical thinking — a pattern we’ve observed as complacency sets in with prolonged use of coding assistants.</p>
<p>The rise of coding agents further amplifies these risks, since AI now generates larger change sets that are harder to review. As with any system, speeding up one part of the workflow increases pressure on the others. Our teams are finding that using AI effectively in production requires renewed focus on code quality. We recommend reinforcing established practices such as TDD and static analysis, and embedding them directly into coding workflows, for example through <a href=""/radar/techniques/curated-prompt-libraries-for-software-teams"">curated shared instructions for software teams</a>.</p>"
Naive API-to-MCP conversion,Hold,Techniques,TRUE,new,"<p>Organizations are eager to let AI agents interact with existing systems, often by attempting a seamless, direct conversion of internal APIs to the <a href=""/radar/platforms/model-context-protocol-mcp"">Model Context Protocol (MCP)</a>. A growing number of tools, such as <a href=""https://github.com/automation-ai-labs/mcp-link"">MCP link</a> and <a href=""https://github.com/tadata-org/fastapi_mcp"">FastAPI-MCP</a>, aim to support this conversion.</p>
<p>We advise against this <strong>naive API-to-MCP conversion</strong>. APIs are typically designed for human developers and often consist of granular, atomic actions that, when chained together by an AI, can lead to excessive token usage, context pollution, and poor agent performance. In addition, these APIs — especially internal ones — frequently expose sensitive data or allow destructive operations. For human developers, such risks are mitigated through architecture patterns and code reviews, but when APIs are naively exposed to agents via MCP, there’s no reliable, deterministic way to prevent an autonomous AI agent from misusing such endpoints. We recommend architecting a dedicated, secure MCP server specifically tailored for agentic workflows, built on top of your existing APIs.</p>"
Standalone data engineering teams,Hold,Techniques,TRUE,new,"<p>Organizing <strong>separate data engineering teams</strong> to develop and own data pipelines and products — separate from the <a href=""https://teamtopologies.com/key-concepts"">stream-aligned</a> business domains they serve — is an anti-pattern that leads to inefficiencies and weak business outcomes. This structure repeats past mistakes of isolating <a href=""/radar/techniques/separate-devops-team"">DevOps</a>, <a href=""/radar/techniques/testing-as-a-separate-organization"">testing</a> or <a href=""/radar/techniques/separate-code-and-pipeline-ownership"">deployment</a> functions, creating knowledge silos, bottlenecks and wasted effort. Without close collaboration, data engineers  often lack the business and domain context needed to design meaningful data products, limiting both adoption and value. In contrast, data platform teams should focus on maintaining the shared infrastructure, while cross-functional business teams build and own their <a href=""/radar/techniques/data-product-thinking"">data products</a>, following <a href=""/radar/techniques/data-mesh"">data mesh</a> principles. We put this practice in Hold to strongly discourage siloed organizational patterns — especially as the need for domain-rich, AI-ready data continues to grow.</p>"
Text to SQL,Hold,Techniques,FALSE,moved out,"<p><strong>Text to SQL</strong> uses LLMs to translate natural language into executable SQL, but its reliability often falls short of expectations. We’ve moved this blip to Hold to discourage its use in unsupervised workflows — for example, dynamically converting user-generated queries where the output is hidden or automated. In these cases, LLMs frequently hallucinate due to limited schema or domain understanding, risking incorrect data retrieval or unintended data modification. The non-deterministic nature of LLM outputs also makes debugging and auditing errors challenging.</p>
<p>We advise treating <a href=""https://davidsj.substack.com/p/a-darker-truth"">Text to SQL with caution</a>. Require human review of generated queries. For agentic business intelligence, avoid direct database access and instead use a governed data abstraction semantic layer — such as <a href=""https://github.com/cube-js/cube"">Cube</a> or <a href=""/radar/languages-and-frameworks/dbt"">dbt</a>'s semantic layer — or a semantically rich access layer like <a href=""​​/radar/techniques/graphql-as-data-access-pattern-for-llms"">GraphQL</a> or <a href=""/radar/platforms/model-context-protocol-mcp"">MCP</a>.</p>"
Arm in the cloud,Adopt,Platforms,FALSE,moved in,"<p><strong><a href=""https://www.arm.com/markets/computing-infrastructure/cloud-computing"">Arm compute instances in the cloud</a></strong> have become increasingly popular in recent years for their cost and energy efficiency compared to traditional x86-based instances. Major cloud providers — including <a href=""https://aws.amazon.com/ec2/graviton/"">AWS</a>, <a href=""https://azure.microsoft.com/blog/azure-virtual-machines-with-ampere-altra-arm-based-processors-generally-available/"">Azure</a> and <a href=""https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu"">GCP</a> — now offer robust Arm options. These instances are especially attractive for large-scale or cost-sensitive workloads. Many of our teams have successfully migrated workloads such as microservices, open-source databases and even high-performance computing to Arm with minimal code changes and only minor build-script adjustments. New cloud-based applications and systems increasingly default to <strong>Arm in the cloud</strong>. Based on our experience, we recommend Arm compute instances for most workloads unless specific architecture dependencies exist. Modern tooling, such as <a href=""https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/"">multi-arch Docker images</a>, further simplifies building and deploying across both Arm and x86 environments.</p>"
Apache Paimon,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/apache/paimon"">Apache Paimon</a></strong> is an open-source data lake format designed to enable <a href=""/radar/techniques/lakehouse-architecture"">lakehouse architecture</a>. It integrates seamlessly with processing engines like <a href=""/radar/platforms/apache-flink"">Flink</a> and <a href=""/radar/platforms/apache-spark"">Spark</a>, supporting both streaming and batch operations. A key advantage of Paimon's architecture lies in its fusion of a standard data lake format with an <a href=""https://en.wikipedia.org/wiki/Log-structured_merge-tree"">LSM (Log-Structured Merge Tree) structure</a>. This combination addresses the traditional challenges of high-performance updates and low-latency reads in data lakes. Paimon supports primary key tables for high-throughput, real-time updates and includes a customizable merge engine for deduplication, partial updates and aggregations. This design enables efficient streaming data ingestion and management of mutable state directly within the lake. Paimon also provides mature data lake capabilities such as scalable metadata, ACID transactions, time travel, schema evolution and optimized data layouts through compression and Z-ordering. We recommend evaluating Paimon for projects that need a unified storage layer capable of efficiently handling large-scale append-only data and complex, real-time streaming updates.</p>"
DataDog LLM Observability,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://www.datadoghq.com/product/llm-observability/"">Datadog LLM Observability</a></strong> provides end-to-end tracing, monitoring and diagnostics for large language models and agentic application workflows. It maps each prompt, tool call and intermediate step into spans and traces; tracks latency, token usage, errors and quality metrics; and integrates with Datadog’s broader APM and observability suite.</p>
<p>Organizations already using Datadog — and familiar with its cost structure — may find the LLM observability feature a straightforward way to gain visibility into AI workloads, assuming those workloads can be instrumented. However, configuring and using LLM instrumentation requires care and a solid understanding of both the workloads and their implementation. We recommend data engineers and operations staff collaborate closely when deploying it. See also our advice on avoiding <a href=""/radar/techniques/separate-data-engineering-teams"">separate data engineering teams</a>.</p>"
Delta Sharing,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://delta.io/sharing/"">Delta Sharing</a></strong> is an open standard and protocol for secure, cross-platform data sharing, developed by <a href=""https://www.databricks.com/product/delta-sharing"">Databricks</a> and the <a href=""https://training.linuxfoundation.org/"">Linux Foundation</a>. It’s cloud-agnostic, enabling organizations to share live data across cloud providers and on-prem locations without copying or replicating the data — preserving data freshness and eliminating duplication costs. We've seen an e-commerce company successfully use Delta Sharing to replace a fragmented partner data-sharing system with a centralized, real-time and secure platform, significantly improving collaboration. The protocol uses a simple REST API to issue short-lived pre-signed URLs, allowing recipients to retrieve large datasets using tools such as pandas, Spark or Power BI. It supports sharing data tables, views, AI models and notebooks. While it provides strong centralized governance and auditing, users should remain mindful of cloud egress costs, which can become a significant operational risk if unmanaged.</p>"
Dovetail Platform,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://dovetail.com/"">Dovetail platform</a></strong> addresses the persistent challenge of managing fragmented qualitative research data. It provides a centralized repository for user interviews, transcripts and insights, turning raw data into a structured, analyzable asset. We’ve found it invaluable in product discovery workflows, particularly for creating an evidence trail that links customer quotes and synthesized themes directly to product hypotheses and estimated ROI. In doing so, Dovetail strengthens the role of qualitative data in product decision-making.</p>"
Langdock AI,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://www.langdock.com/"">Langdock</a></strong> is a platform for organizations to develop and run generative AI agents and workflows for internal operations. It provides a unified environment with internal chat assistants, an API layer for connecting to multiple LLMs and tools for building agentic workflows that integrate with systems such as Slack, Confluence and Google Drive. The platform emphasizes data sovereignty, offering on-premise and EU-hosted options with enterprise compliance standards.</p>
<p>Organizations deploying Langdock should still pay close attention to data governance, and use techniques such as <a href=""https://www.thoughtworks.com/radar/techniques/toxic-flow-analysis-for-ai"">toxic flow analysis</a> to avoid the <a href=""https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/"">lethal trifecta</a>. Adopters should also consider the platform’s maturity, evaluate the specific integrations they require, and plan for any custom development that may be necessary.</p>"
LangSmith,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://www.langchain.com/langsmith"">LangSmith</a></strong> is a hosted platform from the LangChain team that provides observability, tracing and evaluation for LLM applications. It captures detailed traces of chains, tools and prompts, enabling teams to debug and measure model behavior, track performance regressions and manage evaluation data sets. LangSmith is a proprietary SaaS service with limited support for non-LangChain workflows, making it most appealing to teams already invested in that ecosystem. Its integrated support for prompt evaluation and experimentation is notably more polished than open-source alternatives like <a href=""/radar/platforms/langfuse"">Langfuse</a>.</p>"
Model Context Protocol (MCP),Trial,Platforms,FALSE,moved in,"<p>The <strong><a href=""https://modelcontextprotocol.io/"">Model Context Protocol (MCP)</a></strong> is an open standard that defines how LLM applications and agents integrate with external data sources and tools, significantly improving the quality of AI-generated outputs. MCP focuses on context and tool access, distinguishing it from the <a href=""/radar/platforms/agent-to-agent-a2a-protocol"">Agent2Agent (A2A)</a> protocol, which governs inter-agent communication. It specifies servers (for data and tools such as databases, wikis and services) and clients (agents, applications and coding assistants). Since our last blip, MCP adoption has surged, with major companies such as JetBrains (IntelliJ) and Apple joining the ecosystem, alongside emerging frameworks like <a href=""/radar/languages-and-frameworks/fastmcp"">FastMCP</a>. A <a href=""https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/"">preview MCP Registry</a> standard now supports public and proprietary tool discovery. However, MCP's rapid evolution has also introduced architectural gaps, <a href=""https://julsimon.medium.com/why-mcps-disregard-for-40-years-of-rpc-best-practices-will-burn-enterprises-8ef85ce5bc9b"">drawing criticism</a> for overlooking established RPC best practices. For production applications, teams should look <a href=""https://www.thoughtworks.com/insights/blog/generative-ai/model-context-protocol-beneath-hype"">beyond the hype</a> and apply additional scrutiny by mitigating <a href=""/radar/techniques/toxic-flow-analysis-for-ai"">toxic flows</a> using tools like <a href=""/radar/tools/mcp-scan"">MCP-Scan</a> and closely monitoring the <a href=""https://modelcontextprotocol.io/specification/draft/basic/authorization"">draft Authorization module</a> for security.</p>"
n8n,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/n8n-io/n8n"">n8n</a></strong> is a fair-code–licensed workflow automation platform, similar to <a href=""https://zapier.com/"">Zapier</a> or <a href=""https://www.make.com/"">Make</a> (formerly Integromat), but built for developers who want a self-hosted, extensible and code-controllable option. It offers a lower-code, visual approach to workflow creation than <a href=""https://airflow.apache.org/"">Apache Airflow</a>, while still supporting custom code in JavaScript or Python.</p>
<p>Its primary use case is integrating multiple services into automated workflows, but it can also connect LLMs with configurable data sources, memory and tools. Many of our teams use n8n to rapidly prototype agentic workflows triggered by chat applications or webhooks, often leveraging its import and export capabilities to generate workflows with AI assistance. As always, we advise caution when using <a href=""/radar/platforms/low-code-platforms"">low-code platforms</a> in production. However, n8n's self-hosting and code-defined workflows can mitigate some of those risks.</p>"
OpenThread,Trial,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/openthread/openthread"">OpenThread</a></strong> is an open-source implementation of the <a href=""https://www.threadgroup.org/What-is-Thread/Overview"">Thread</a> networking protocol developed by Google. It supports all key features of the Thread specification — including networking layers such as IPv6, 6LoWPAN and LR-WPAN — as well as mesh network capabilities that allow a device to function as both a node and a border router. OpenThread runs on a wide range of hardware platforms, leveraging a flexible abstraction layer and integration hooks that enable vendors to incorporate their own radio and cryptographic capabilities. This mature protocol is widely used in commercial products and, in our experience, has proven reliable for building diverse IoT solutions — from battery-operated, low-power devices to large-scale mesh sensor networks.</p>"
AG-UI Protocol,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/ag-ui-protocol/ag-ui"">AG-UI</a></strong> is an open protocol and library designed to standardize communication between rich user interfaces and agents. Focused on direct user-facing agents, it uses middleware and client integrations to generalize across any frontend and backend. The protocol defines a consistent way for back-end agents to communicate with front-end applications, enabling real-time, stateful collaboration between AI and human users. It supports multiple transport protocols, including SSE and WebSockets, and provides standardized event types to represent different states of agent execution. Built-in support is available for popular agentic frameworks such as <a href=""/radar/languages-and-frameworks/langgraph"">LangGraph</a> and <a href=""/radar/languages-and-frameworks/pydanticai"">Pydantic AI</a>, with community integrations for others.</p>"
Agent-to-Agent (A2A) Protocol,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/a2aproject/A2A"">Agent2Agent (A2A)</a></strong> is a protocol that defines a standard for communication and interaction among agents in complex, multi-agent workflows. It uses <em>Agent Cards</em> to describe the key elements of inter-agent communication, including skill discovery and the specification of transport and security schemes. A2A complements the <a href=""/radar/platforms/model-context-protocol-mcp"">Model Context Protocol</a> (MCP) by focusing on agent-to-agent communication without exposing internal details such as an agent's state, memory or internal. </p>
<p>The protocol promotes best practices such as an asynchronous-first approach for long-running tasks, streaming responses for incremental updates and secure transport with HTTPS, authentication and authorization. SDKs are available in Python, JavaScript, Java and C# to facilitate rapid adoption. Although relatively new, A2A enables teams to build domain-specific agents that can collaborate to form complex workflows, making it a strong option for such scenarios.</p>"
Amazon S3 Vectors,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://aws.amazon.com/s3/features/vectors/"">Amazon S3 Vectors</a></strong> extends the S3 object store with native vector capabilities, offering built-in vector storage and similarity search functionality. It integrates seamlessly with the AWS ecosystem, including Amazon Bedrock and OpenSearch, and provides additional features such as metadata filtering and governance via IAM. While still in preview and subject to <a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors-limitations.html"">restrictions and limitations</a>, we find its value proposition compelling. This cost-effective, accessible approach to vector storage could enable a range of applications that involve large data volumes and where low latency is not the primary concern.</p>"
Ardoq,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://www.ardoq.com/"">Ardoq</a></strong> is an enterprise architecture (EA) platform that enables organizations to build, manage and scale their architecture knowledge bases so they can plan more effectively for the future. Unlike traditional static documentation, which is prone to drift and siloing, Ardoq's data-driven approach pulls information from existing systems to create a dynamic knowledge graph that stays up to date as the landscape evolves. One feature we've found particularly useful is Ardoq Scenarios, which allows you to visually model and define <em>what-if</em> future states using a branching and merging approach similar to Git. Organizations pursuing architectural transformation should assess dedicated EA platforms like Ardoq for their potential to streamline and accelerate this process.</p>"
CloudNativePg,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://cloudnative-pg.io/"">CloudNativePG</a></strong> is a <a href=""/radar/tools/kubernetes-operators"">Kubernetes Operator</a> that simplifies hosting and managing highly available PostgreSQL clusters in Kubernetes. Running a stateful service like PostgreSQL on Kubernetes can be complex, requiring deep knowledge of both Kubernetes and PostgreSQL replication. CloudNativePG abstracts much of this complexity by treating the entire PostgreSQL cluster as a single, configurable declarative resource. It provides seamless primary/standby architecture using native streaming replication and includes high-availability features out of the box, including self-healing capabilities, automated failover that promotes the most aligned replica and automatic recreation of failed replicas. If you’ are looking to host PostgreSQL on Kubernetes, CloudNativePG is a solid place to start.</p>"
coder.com,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://coder.com/"">Coder.com</a></strong> is a platform for quickly provisioning standardized coding environments, following the <a href=""/radar/techniques/development-environments-in-the-cloud"">development environments in the cloud</a> practice we've described before. Compared with similar tools such as <a href=""/radar/tools/gitpod"">Gitpod</a> (now rebranded as <a href=""https://ona.com/"">Ona</a>) and <a href=""/radar/tools/github-codespaces"">GitHub Codespaces</a>, Coder.com offers greater control over workstation customization through Terraform. It hosts workstations on your own infrastructure, whether in the cloud or in a data center, rather than on a vendor's servers. This approach provides more flexibility, including the ability to run AI coding agents and access internal organizational systems. However, this flexibility comes with tradeoffs: More effort to set up and maintain workstation templates and greater responsibility for managing data security risks in agentic workflows.</p>"
Graft,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/orbitinghail/graft"">Graft</a></strong> is a transactional storage engine that enables strongly consistent and efficient data synchronization across edge and distributed environments. It achieves this by using lazy replication to sync data only on demand, partial replication to minimize bandwidth consumption and serializable snapshot isolation to guarantee data integrity. We’ve mentioned <a href=""/radar/languages-and-frameworks/electric"">Electric</a> in the Radar for a similar use case, but we see Graft as unique in turning object storage into a transactional system that supports consistent page-level updates without imposing a data format. This makes it well-suited to powering <a href=""/radar/techniques/local-first-application"">local-first</a> mobile applications, managing complex cross-platform synchronization and serving as the backbone for stateless replicas in serverless or embedded systems.</p>"
groundcover,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://www.groundcover.com/"">groundcover</a></strong> is a cloud-native observability platform that unifies logs, traces, metrics and Kubernetes events in a single pane of glass. It leverages <a href=""radar/platforms/ebpf"">eBPF</a> to capture granular observability data with zero code instrumentation — that is, without inserting agents or SDKs into application code. groundcover’s eBPF sensor runs on a dedicated node in each monitored cluster, operating independently from the applications it observes. Key features include deep kernel-level visibility, a <a href=""https://docs.groundcover.com/?_gl=1*ruvtje*_gcl_au*OTE5OTA1MzMuMTc1OTgxNjg4NQ..#bring-your-own-cloud-byoc-architecture"">bring-your-own-cloud (BYOC) architecture</a> for data privacy and a data volume–agnostic pricing model that keeps costs predictable.</p>"
Karmada,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://karmada.io/"">Karmada</a></strong> (""Kubernetes Armada"") is a platform for orchestrating workloads across multiple Kubernetes clusters, clouds and data centers. Many teams currently deploy across clusters using <a href=""/radar/techniques/gitops"">GitOps</a> tools like <a href=""https://fluxcd.io/"">Flux</a> or <a href=""/radar/platforms/argo-cd"">ArgoCD</a> combined with custom scripts, so a purpose-built solution is welcome. Karmada leverages Kubernetes-native APIs, requiring no changes to applications already built for cloud-native environments. It offers advanced scheduling capabilities for multi-cloud management, high availability, failure recovery and traffic scheduling.</p>
<p>Karmada is still relatively new, so it's important to assess the maturity of the features your team depends on. As a CNCF project, however, it has strong momentum, and several of our teams are already using it successfully. Note that certain areas — such as networking, state and storage management across clusters — are outside Karmada’s scope. Most teams will still need a <a href=""/radar/techniques/service-mesh"">service mesh</a> like <a href=""/radar/platforms/istio"">Istio</a> or <a href=""https://linkerd.io/"">Linkerd</a> for traffic handling and should plan how to manage stateful workloads and distributed data.</p>"
OpenFeature,Assess,Platforms,TRUE,new,"<p>As businesses scale, feature flag management often becomes increasingly complex; teams need an abstraction layer that goes beyond the <a href=""/radar/techniques/simplest-possible-feature-toggle"">simplest possible feature toggle</a>. <strong><a href=""https://openfeature.dev/"">OpenFeature</a></strong> provides this layer through a vendor-agnostic, community-driven API specification that standardizes how feature flags are defined and consumed, decoupling application code from the management solution. This flexibility allows teams to switch providers easily — from basic setups using environment variables or in-memory configurations up to mature platforms like <a href=""/radar/tools/configcat"">ConfigCat</a> or <a href=""https://launchdarkly.com/"">LaunchDarkly</a>. However, one critical caution remains: teams must manage different <a href=""https://martinfowler.com/articles/feature-toggles.html#CategoriesOfToggles"">categories of flags</a> separately and with discipline to avoid flag proliferation, application complexity and excessive testing overhead.</p>"
Oxide,Assess,Platforms,TRUE,new,"<p>Building and operating private infrastructure is complex. That’s one of the main reasons public cloud is the default for most organizations. However, for those that need it, <strong><a href=""https://oxide.computer/"">Oxide</a></strong> offers an alternative to assembling and integrating hardware and software from scratch. It provides prebuilt racks with compute, networking and storage, running fully integrated system software. Teams can manage resources through Oxide’s IaaS APIs using Terraform and other automation tools — what Oxide calls <em>on-premises elastic infrastructure</em>.</p>
<p>Dell and VMware’s <a href=""https://www.dell.com/en-uk/dt/converged-infrastructure/vxrail/index.htm"">VxRail</a>, <a href=""https://www.nutanix.com/"">Nutanix</a> and <a href=""https://www.hpe.com/us/en/storage/simplivity.html"">HPE SimpliVity</a> also provide hyper-converged infrastructure (HCI) solutions, but what distinguishes Oxide is its purpose-built approach. It designs the entire stack — from circuit boards and power supplies to firmware — instead of assembling components from different vendors. Oxide has also developed and open-sourced <a href=""https://github.com/oxidecomputer/hubris"">Hubris</a>, a lightweight, memory-protected, message-passing kernel written in Rust for embedded systems, along with other Rust-based infrastructure projects. We also appreciate that Oxide sells their equipment and software without license fees.</p>"
Restate,Assess,Platforms,FALSE,no change,"<p><strong><a href=""https://restate.dev"">Restate</a></strong> is a durable execution platform designed to address complex distributed system challenges when building stateful, fault-tolerant applications. It logs every step via execution journaling, ensuring fault-tolerance, reliable recovery and exactly-once communication across services. The platform’s key architectural advantage lies in separating application logic into three durable service types: Basic Services for stateless functions; Virtual Objects to model concurrent, stateful entities; and Workflows to orchestrate complex, multi-step processes. We’ve been carefully assessing Restate in a large insurance system and are quite happy with its performance so far.</p>"
SkyPilot,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/skypilot-org/skypilot"">SkyPilot</a></strong> is an open-source platform for running and scaling AI workloads on-premises or in the cloud. Developed by the Sky Computing Lab at UC Berkeley, SkyPilot acts as an intelligent broker, automatically finding and provisioning the cheapest, most available GPUs across major clouds and Kubernetes clusters, often cutting compute costs. For infrastructure teams, it simplifies running AI on Kubernetes by offering <a href=""https://en.wikipedia.org/wiki/Slurm_Workload_Manager"">Slurm</a>-like ease of use, cloud-native robustness, direct SSH access to pods and features such as <a href=""https://en.wikipedia.org/wiki/Gang_scheduling"">gang scheduling</a> and multi-cluster support for seamless scaling of training or inference workloads.</p>"
StarRocks,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://www.starrocks.io/"">StarRocks</a></strong> is an analytical database that redefines real-time business intelligence by combining the speed of traditional OLAP systems with the flexibility of a modern <a href=""/radar/techniques/lakehouse-architecture"">data lakehouse</a>. It achieves sub-second query latency at massive scale through a SIMD-optimized execution engine, columnar storage and a sophisticated cost-based optimizer. This high-performance architecture allows users to run complex analytics directly on open data formats such as <a href=""/radar/platforms/apache-iceberg"">Apache Iceberg</a>, without pre-computation or data copying. While there are many platforms in this space, we see StarRocks as a strong candidate for cost-effective solutions that require both extreme concurrency and consistent, up-to-the-second data freshness.</p>"
Uncloud,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/psviderski/uncloud"">Uncloud</a></strong> is a lightweight container orchestration and clustering tool that enables developers to take Docker Compose applications to production, offering a simple, cloud-like experience without the operational overhead of Kubernetes. It achieves cross-machine scaling and zero-downtime deployments by automatically configuring a secure <a href=""https://www.wireguard.com/"">WireGuard</a> mesh network for communication and using the <a href=""https://caddyserver.com/"">Caddy</a> reverse proxy to provide automatic HTTPS and load balancing. Uncloud’s main architectural advantage is its fully decentralized design, which eliminates the need for a central control plane and ensures cluster operations remain functional even if individual machines go offline. With Uncloud, you can freely mix and match cloud VMs and bare-metal servers into a unified and cost-effective computing environment.</p>"
ClickHouse,Adopt,Tools,FALSE,moved in,"<p><strong><a href=""https://clickhouse.com/"">ClickHouse</a></strong> is an open-source, distributed columnar online analytical processing (OLAP) database for real-time analytics. It has matured into a highly performant and scalable engine capable of handling large-scale data analytics. Its incremental materialized view, efficient query engine and strong data compression make it ideal for interactive queries. Built-in support for approximate aggregate functions enables trade-offs between accuracy and performance, which is especially useful for high-cardinality analytics. The addition of the S3 storage engine and MergeTree allows separation of storage and compute, using S3-compatible storage for ClickHouse tables. We’ve also found ClickHouse to be an excellent backend for <a href=""https://clickhouse.com/blog/clickhouse-and-open-telemtry"">OpenTelemetry</a> data and crash analytics tools like <a href=""/radar/tools/sentry"">Sentry</a>. For teams seeking a fast, open-source analytics engine, ClickHouse is an excellent choice.</p>"
NeMo Guardrails,Adopt,Tools,FALSE,moved in,"<p><strong><a href=""https://github.com/NVIDIA-NeMo/Guardrails"">NeMo Guardrails</a></strong> is an open-source toolkit from NVIDIA that makes it easy to add programmable safety and control mechanisms to LLM-based conversational applications. It ensures outputs remain safe, on-topic and compliant by defining and enforcing behavioral rules. Developers use Colang, a purpose-built language, to create flexible dialogue flows and manage conversations, enforcing predefined paths and operational procedures. NeMo Guardrails also provides an asynchronous-first API for performance and supports safeguards for content safety, security and moderation of inputs and outputs. We’re seeing steady adoption across teams building applications that range from simple chatbots to complex agentic workflows. With its expanding feature set and maturing coverage of common LLM vulnerabilities, we’re moving NeMo Guardrails to Adopt.</p>"
pnpm,Adopt,Tools,FALSE,moved in,"<p>Since the last Radar, we’ve continued to receive positive feedback about <strong><a href=""https://pnpm.js.org/"">pnpm</a></strong> from teams. pnpm is a <a href=""/radar/platforms/node-js"">Node.js</a> package manager that delivers significant performance improvements over alternatives, both in speed and disk space efficiency. It hard-links duplicate packages from multiple projects’ <code>node_modules</code> folders to a single location on disk and supports incremental, file-level optimizations that further boost performance. Because pnpm offers a much faster feedback loop with minimal compatibility issues, it has become our default choice for Node.js package management.</p>"
Pydantic,Adopt,Tools,FALSE,moved in,"<p><strong><a href=""https://docs.pydantic.dev/latest/"">Pydantic</a></strong> is a Python library that uses standard type hints to define data models and enforce data schemas at runtime. Originally, type annotations were added to Python for static analysis, but their growing versatility has led to broader uses, including runtime validation. Built on a fast Rust core, it provides efficient data validation, parsing and serialization.</p>
<p>While it’s best known for web API development, Pydantic has also become essential in LLM applications. We typically use the <a href=""/radar/techniques/structured-output-from-llms"">structured output from LLMs</a> technique to manage the unpredictable nature of LLMs. By defining a strict data schema, it acts as a safety net for the unpredictable nature of model output — converting free-form text responses into deterministic, type-safe Python objects (e.g., JSON). This approach, often implemented through <a href=""/radar/languages-and-frameworks/pydanticai"">PydanticAI</a> or <a href=""https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html"">LangChain</a>, turns potentially brittle LLM interactions into reliable, machine-readable data contracts. Our teams have successfully used Pydantic in production to extract structured representations from unstructured documents, ensuring the output conforms to a valid structure. Given its maturity, performance and reliability, Pydantic is now our default choice for production-level Python AI applications.</p>"
AI Design reviewer,Trial,Tools,TRUE,new,"<p><strong><a href=""https://www.figma.com/community/plugin/1339202278007297015/ai-design-reviewer-detect-ui-ux-accessibility-copy-issues"">AI Design Reviewer</a></strong> is a <a href=""/radar/tools/figma"">Figma</a> plugin for conducting design audits or heuristic evaluations and collecting actionable feedback on existing or new designs. Its audits cover UX critiques, UI inconsistencies, accessibility gaps, content quality and edge-case scenarios. Beyond identifying issues, it provides domain-aware recommendations that help teams build a shared design vocabulary and rationale behind design choices. Our teams used AI Design Reviewer to analyze legacy designs — identifying positive experiences to retain and negative ones to address — which informed UX goals for redesigns. It has also served as a peer-review substitute, offering early feedback on new designs before development handoff.</p>"
Barman,Trial,Tools,TRUE,new,"<p><strong><a href=""https://github.com/EnterpriseDB/barman"">Barman</a></strong> (Backup and Recovery Manager) is an open-source tool for managing backups and disaster recovery of PostgreSQL servers. It supports the full disaster recovery process, simplifying the creation of physical backups through a <a href=""https://docs.pgbarman.org/release/3.15.0/user_guide/concepts.html"">variety of methods</a>, organizing them into a comprehensive <a href=""https://docs.pgbarman.org/release/3.15.0/user_guide/catalog.html"">catalog</a> and restoring backups to a live server with point-in-time recovery capabilities. We’ve found Barman to be powerful and easy to use, and have been impressed at the speed of point-in-time recovery operations during migration activities. We've also found it capable for scheduled backups, with an ability to handle complex, mixed configurations of scheduling and retention.</p>"
Claude Code,Trial,Tools,TRUE,new,"<p>Anthropic's <strong><a href=""https://claude.com/product/claude-code"">Claude Code</a></strong> is an agentic AI coding tool that provides a natural language interface and agentic execution model for planning and implementing complex, multi-step workflows. Released less than a year ago, it has already been widely adopted by developers inside and outside Thoughtworks, leading us to place it in Trial. Console-based coding agents such as OpenAI's <a href=""https://developers.openai.com/codex/cli/"">Codex CLI</a>, Google's <a href=""https://developers.google.com/gemini-code-assist/docs/gemini-cli"">Gemini CLI</a> and the open-source <a href=""https://apidog.com/blog/opencode/"">OpenCode</a> have been released, while IDE-based assistants like <a href=""/radar/tools/cursor"">Cursor</a>, <a href=""/radar/tools/windsurf"">Windsurf</a> and <a href=""/radar/tools/github-copilot"">GitHub Copilot</a> now include agent modes. Even so, Claude Code remains a favorite. We see teams using it not only to write and modify code but also as a general-purpose AI agent for managing specifications, stories, configuration, infrastructure and documentation.</p>
<p>Agentic coding shifts the developer's focus from writing code to specifying intent and delegating implementation. While this can accelerate development cycles, it can also lead to <a href=""/radar/techniques/complacency-with-ai-generated-code"">complacency with AI-generated code</a>, which in turn may result in code that is harder to maintain and evolve — for both humans and AI agents. It’s therefore essential for teams to rigorously manage how Claude Code works, using techniques such as <a href=""/radar/techniques/context-engineering"">context engineering</a>, <a href=""/radar/techniques/curated-prompt-libraries-for-software-teams"">curated shared instructions</a> and potentially <a href=""/radar/techniques/team-of-coding-agents"">teams of coding agents</a>.</p>"
Cleanlab,Trial,Tools,TRUE,new,"<p>In the data-centric AI paradigm, improving dataset quality often delivers greater performance gains than tuning the model itself. <strong><a href=""https://github.com/cleanlab/cleanlab"">Cleanlab</a></strong> is an open-source Python library designed to address this challenge by automatically identifying common data issues — such as mislabeling, outliers and duplicates — across text, image, tabular and audio data sets. Built on the principle of <a href=""https://arxiv.org/abs/1911.00068"">confident learning</a>, Cleanlab leverages model-predicted probabilities to estimate label noise and quantify data quality.</p>
<p>This model-agnostic approach enables developers to diagnose and correct data set errors, then retrain models for improved robustness and accuracy. Our teams have used Cleanlab successfully in production, confirming its effectiveness in real-world settings. We recommend it as a valuable tool for promoting data standardization and improving data set quality in AI engineering projects.</p>"
Context7,Trial,Tools,TRUE,new,"<p><strong><a href=""https://github.com/upstash/context7"">Context7</a></strong> is an <a href=""/radar/platforms/model-context-protocol-mcp"">MCP</a> server that addresses inaccuracies in AI generated code. While LLMs rely on outdated training data, Context7 ensures they generate accurate, up-to-date and version-specific code for the libraries and frameworks used in a project. It does this by pulling the latest documentation and functional code examples directly from framework source repositories and injecting them into the LLM's context window at the moment of prompting. In our experience, Context7 has greatly reduced code hallucinations and reliance on stale training data. You can configure it with AI code editors such as Claude Code, Cursor or VS Code to generate, refactor or debug framework-dependent code.</p>"
Data Contract CLI,Trial,Tools,TRUE,new,"<p><strong><a href=""https://gpt.datacontract.com/sources/cli.datacontract.com/"">Data Contract CLI</a></strong> is an open-source command-line tool designed for working with the <a href=""https://datacontract.com/"">Data Contract</a> specification. It helps you create and edit data contracts and, critically, lets you validate data against its contract, which is essential for ensuring the integrity and quality of your data products.</p>
<p>The CLI offers broad support for multiple schema definitions (Avro, SQL DDL, <a href=""https://bitol-io.github.io/open-data-contract-standard/v3.0.2/"">Open Data Contract Standard</a>, etc.) and can compare different contract versions to immediately detect breaking changes. We've found it especially useful in the data mesh space to operationalize contract governance between data products via CI/CD integration. This approach reduces manual errors and ensures data quality, integrity and compatibility in data exchanges across services.</p>"
Databricks Assistant,Trial,Tools,TRUE,new,"<p><strong><a href=""https://www.databricks.com/product/databricks-assistant"">Databricks Assistant</a></strong> is an AI-powered conversational tool integrated directly into the Databricks platform, acting as a contextual pair programmer for data professionals. Unlike general-purpose coding assistants, it benefits from a native understanding of the Databricks environment and data context, including metadata from the Unity Catalog. The Assistant goes beyond generating code snippets; it can craft complex, multi-step SQL and Python queries, diagnose errors and provide detailed, workspace-specific explanations. For organizations already invested in the Databricks ecosystem, it can accelerate productivity and lower the barrier to entry for complex data tasks.</p>"
Hoppscotch,Trial,Tools,TRUE,new,"<p><strong><a href=""https://hoppscotch.com/"">Hoppscotch</a></strong> is a lightweight open-source tool for API development, debugging, testing and sharing. It supports multiple protocols — including HTTP, GraphQL and WebSocket — and offers cross-platform clients for web, desktop and CLI environments.</p>
<p>While the API tooling space is crowded with alternatives like <a href=""/radar/tools/postman"">Postman</a>, <a href=""/radar/tools/insomnia"">Insomnia</a> and <a href=""radar/tools/bruno"">Bruno</a>, Hoppscotch stands out for its lightweight footprint and privacy-friendly design. It omits analytics, uses local-first storage and supports self-hosting. It’s a strong choice for organizations seeking an intuitive way to share API scripts while maintaining strong data privacy.</p>"
NVIDIA DCGM explorer,Trial,Tools,TRUE,new,"<p><strong><a href=""https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/latest/dcgm-exporter.html"">NVIDIA DCGM Exporter</a></strong> is an open-source tool that helps teams monitor distributed GPU training at scale. It converts proprietary telemetry from the <a href=""https://developer.nvidia.com/dcgm"">NVIDIA Data Center GPU Manager (DCGM)</a> into open formats compatible with standard monitoring systems. The Exporter exposes critical real-time metrics — including GPU utilization, temperature, power and ECC error counts—from both GPU and host servers. This visibility is essential for organizations fine-tuning custom LLMs or running long-duration, GPU-intensive training jobs. The straggler effect — where one slow worker bottlenecks the entire process —  can <a href=""https://arxiv.org/html/2505.05713v2"">reduce throughput</a> by over 10% and waste up to 45% of allocated GPU hours. Designed for cloud-native, large-scale environments, the DCGM Exporter integrates seamlessly with <a href=""/radar/tools/prometheus"">Prometheus</a> and <a href=""/radar/tools/grafana"">Grafana</a>, helping ensure every GPU operates within optimal performance bounds.</p>"
Relational AI,Trial,Tools,TRUE,new,"<p>When large volumes of diverse data are brought into Snowflake, the inherent relationships and implicit rules within that data can become obscured. Built as a Snowflake Native App, <strong><a href=""https://www.relational.ai/"">RelationalAI</a></strong> enables teams to build sophisticated models that capture meaningful concepts, define core business entities and embed complex logic directly against Snowflake tables. Its powerful Graph Reasoner allows users to then create, analyze and visualize relational knowledge graphs based on these models. Built-in <a href=""https://docs.relational.ai/build/reasoners/graph/algorithms/#available-algorithms"">algorithms</a> help explore graph structures and reveal hidden patterns. For organizations managing massive, fast-changing data sets, constructing a knowledge graph can be essential for proactive monitoring and generating richer and more actionable insights.</p>"
UXPilot,Trial,Tools,TRUE,new,"<p><strong><a href=""https://uxpilot.ai/"">UX Pilot</a></strong> is an AI tool that supports multiple stages of the UX design process — from wireframing to high-fidelity visual design and review. It accepts text or image inputs and can automatically generate screens, flows and layouts. Its Autoflow feature creates user flow transitions, while Deep Design produces richer, more detailed outputs. UX Pilot also includes a <a href=""/radar/tools/figma"">Figma</a> plugin that exports generated designs for refinement within standard design tools. Our teams have used UX Pilot for ideation and inspiration, generating multiple options during <a href=""https://designsprintkit.withgoogle.com/methodology/phase3-sketch/crazy-8s"">Crazy 8’s exercises</a> and translating project story lists into product vision boards and epic-level design concepts. Tools like UX Pilot also enable non-designers, such as product managers, to <a href=""/radar/techniques/self-serve-ui-prototyping-with-genai"">create quick prototypes</a> and gather early stakeholder feedback — a growing trend in AI-assisted design workflows.</p>"
v0,Trial,Tools,FALSE,moved in,"<p><strong><a href=""https://v0.app/"">v0</a></strong> has evolved since we last featured it in the Radar. It now includes a design mode that further lowers the barrier for product managers to create and tweak <a href=""/radar/techniques/self-serve-ui-prototyping-with-genai"">Self-serve UI prototypes</a>. The latest release introduces an in-house model with large context windows and multimodal capabilities, enabling v0 to generate and improve UIs from both text and visual inputs. Another notable addition is its agentic mode, which allows the system to break down more complex work and select the appropriate model for each. However, this feature is still new, and early feedback has been mixed.</p>"
Augment Code,Assess,Tools,TRUE,new,"<p><strong><a href=""https://www.augmentcode.com/"">Augment Code</a></strong> is an AI coding assistant that delivers deep, context-aware support across large codebases. It stands out through advanced <a href=""/radar/techniques/context-engineering"">context engineering</a> that enables rapid <a href=""https://www.augmentcode.com/blog/a-real-time-index-for-your-codebase-secure-personal-scalable"">code index updates</a> and fast retrieval, even as code changes frequently. Augment supports models such as <a href=""/radar/tools/claude-sonnet"">Claude Sonnet 4 and 4.5</a> and <a href=""https://openai.com/gpt-5/"">GPT-5</a>, integrates with GitHub, Jira and Confluence and supports the <a href=""/radar/platforms/model-context-protocol-mcp"">Model Context Protocol (MCP)</a> for external tool interoperability. It provides turn-by-turn guidance for complex codebase changes — from refactors and dependency upgrades to schema updates — along with personalized in-line completions that reflect project-specific dependencies. Augment also promotes collaboration by allowing teams to query and share code insights directly within Slack.</p>"
Azure AI Document Intelligence,Assess,Tools,TRUE,new,"<p><strong><a href=""https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence"">Azure AI Document Intelligence</a></strong> (formerly Form Recognizer) extracts text, tables and key-value pairs from unstructured documents and transforms them into structured data. It uses pre-trained deep learning models to interpret layouts and semantics, and custom models can be trained through a no-code interface for specialized formats. In some cases, however, power users may require a custom fine-tuning interface instead.</p>
<p>One of our teams reported that ADI significantly reduced manual data entry, improved data accuracy and accelerated reporting, leading to faster data-driven decisions. Like <a href=""https://aws.amazon.com/textract/"">Amazon Textract</a> and <a href=""https://cloud.google.com/document-ai"">Google Document AI</a>, it provides enterprise-grade document processing with strong layout understanding. An emerging open-source alternative is IBM’s Docling, which offers a more flexible, code-centric approach to structured data extraction. Compared to traditional OCR tools, ADI captures not just text but also structure and relationships, making it easy to integrate into downstream data pipelines. That said, we’ve observed occasional latency when embedding it into synchronous user workflows, so we recommend using it primarily for asynchronous processing.</p>"
Docling,Assess,Tools,TRUE,new,"<p><strong><a href=""https://docling-project.github.io/docling/"">Docling</a></strong> is an open-source Python and TypeScript library for advanced document processing of unstructured data. It addresses the often overlooked ""last mile"" problem of converting real-world documents — like PDFs and PowerPoints — into clean, machine-readable formats. Unlike traditional extractors, Docling uses a computer vision–based approach to interpret document layout and semantic structure, which makes its output particularly valuable for <a href=""/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> pipelines. It converts complex documents into structured formats such as JSON or Markdown, supporting techniques like <a href=""/radar/techniques/structured-output-from-llms"">structured output from LLMs</a>. This contrasts with <a href=""/radar/tools/colpali"">ColPali</a>, which feeds page images directly to a vision-language model for retrieval.</p>
<p>Docling's open-source nature and Python core, built on a custom <a href=""/radar/tools/pydantic"">Pydantic</a>-based data model, provide a flexible, self-hosted alternative to proprietary cloud tools such as <a href=""/radar/tools/azure-document-intelligence"">Azure Document Intelligence</a>, <a href=""https://aws.amazon.com/textract/"">Amazon Textract</a> and <a href=""https://cloud.google.com/document-ai"">Google Document AI</a>. Backed by IBM Research, the project’s rapid development and plug-and-play architecture for integrating with other frameworks like <a href=""/radar/languages-and-frameworks/langgraph"">LangGraph</a> make it well worth assessing for teams building production-grade AI-ready data pipelines.</p>"
E2B,Assess,Tools,TRUE,new,"<p><strong><a href=""https://github.com/e2b-dev/E2B"">E2B</a></strong> is an open-source tool for running AI-generated code in secure, isolated sandboxes in the cloud. Agents can use these sandboxes, built on top of <a href=""https://firecracker-microvm.github.io/"">Firecracker</a> microVMs, to safely execute code, analyze data, conduct research or operate a virtual machine. This enables you to build and deploy enterprise-grade AI agents with full control and security over the execution environment.</p>"
Helix editor,Assess,Tools,TRUE,new,"<p>There’s been somewhat of a resurgence in simple text editors aiming to replace the command-line favorite Vim. <strong><a href=""https://helix-editor.com/"">Helix</a></strong> is one such contender in the crowded space alongside <a href=""https://neovim.io/"">Neovim</a> and, more recently, <a href=""https://kakoune.org/"">Kakoune</a>. Describing itself — somewhat playfully — as a post-modern text editor, Helix features multiple cursors, Tree-sitter support and integrated Language Server Protocol (LSP) support, which is what first drew our attention. Helix is actively developed. with a plugin system on the way. Overall, it’s a lightweight modal editor that feels familiar to Vim users but adds a few modern conveniences.</p>"
Kueue,Assess,Tools,TRUE,new,"<p><strong><a href=""https://kueue.sigs.k8s.io/docs/overview/"">Kueue</a></strong> is a Kubernetes-native controller for job queuing that manages quotas and resource consumption. It provides APIs for handling Kubernetes workloads with varying priorities and resource requirements, functioning as a job-level manager that determines when to admit or evict jobs. Designed for efficient resource management, job prioritization and advanced scheduling, Kueue helps optimize workload execution in Kubernetes environments — particularly for ML workloads using tools such as <a href=""https://www.kubeflow.org/"">Kubeflow</a>. It works alongside the cluster-autoscaler and kube-scheduler rather than replacing them, focusing on job admission based on order, quota, priority and <a href=""/radar/techniques/topology-aware-scheduling"">topology awareness</a>. As part of the Kubernetes Special Interest Group (SIG) ecosystem, Kueue adheres to its development standards.</p>"
MCP Scan,Assess,Tools,TRUE,new,"<p><strong><a href=""https://mcpscan.ai/"">MCPScan.ai</a></strong> is a security scanner for <a href=""/radar/platforms/model-context-protocol-mcp"">Model Context Protocol (MCP)</a> servers that operates in two modes: scan and proxy. In scan mode, it analyzes configurations and tool descriptions to detect known vulnerabilities such as prompt injections, <a href=""https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks"">tool poisoning</a> and <a href=""/radar/techniques/toxic-flow-analysis-for-ai"">toxic flows</a>. In proxy mode, MCPScan.ai acts as a bridge between agent system and MCP server, continuously monitoring runtime traffic. This mode also enforces custom security rules and guardrails, including tool call validation, PII detection and data flow constraints. The tool provides a proactive security layer for agents, ensuring that even if a malicious prompt is accepted, the agent cannot execute harmful actions. MCPScan.ai is a purpose-built security solution for the emerging field of agentic systems.</p>"
oRPC,Assess,Tools,TRUE,new,"<p><strong><a href=""https://orpc.unnoq.com/"">oRPC</a></strong> (OpenAPI Remote Procedure Call) provides end-to-end typesafe APIs in TypeScript while fully adhering to the OpenAPI specification. It can automatically generate a complete OpenAPI spec, simplifying integration and documentation. We’ve found oPRC particularly strong for integrations. While alternatives such as <a href=""https://trpc.io/"">tRPC</a> and <a href=""/radar/languages-and-frameworks/elysiajs"">ElysiaJS</a> often require adopting a new framework to gain type safety, oRPC integrates seamlessly with existing Node.js frameworks, including <a href=""https://expressjs.com/"">Express</a>, <a href=""/radar/languages-and-frameworks/fastify"">Fastify</a>, <a href=""https://hono.dev/"">Hono</a> and <a href=""/radar/languages-and-frameworks/next-js"">Next.js</a>. This flexibility makes it an excellent choice for teams looking to adopt end-to-end type safety to existing APIs without a disruptive refactor.</p>"
Power user for dbt VS Code extension,Assess,Tools,TRUE,new,"<p><strong><a href=""https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user"">Power user for dbt</a></strong> is an extension for Visual Studio Code that integrates directly with both <a href=""/radar/languages-and-frameworks/dbt"">dbt</a> and <a href=""https://docs.getdbt.com/docs/dbt-cloud-environments"">dbt Cloud environments</a>. Since dbt remains one of our favourite tools, anything that improves its usability is a welcome addition to the ecosystem. Previously, developers relied on multiple tools to validate SQL code or inspect model lineage outside the IDE. With this extension, those capabilities are now built into VS Code, offering code autocompletion, real-time query results and visual model and column lineage. This last feature makes it easy to navigate between models. Our teams report the plugin reduces pipeline errors and enhances the overall development experience. If you use <strong>dbt</strong>, we urge you to take a look at this tool.</p>"
Serena,Assess,Tools,TRUE,new,"<p><strong><a href=""https://github.com/oraios/serena"">Serena</a></strong> is a powerful coding toolkit that equips coding agents such as Claude Code with IDE-like capabilities for semantic code retrieval and editing. By operating at the symbol level and understanding the relational structure of code, Serena greatly improves token efficiency. Instead of reading entire files or relying on crude string replacements, coding agents can use precise Serena <a href=""https://github.com/oraios/serena?tab=readme-ov-file#list-of-tools"">tools</a> such as find_symbol, find_referencing_symbols and insert_after_symbol to locate and edit code. Although the impact is minimal on small projects, this efficiency is extremely valuable as the codebase grows.</p>"
Sweetpad,Assess,Tools,TRUE,new,"<p>The <strong><a href=""https://github.com/sweetpad-dev/sweetpad"">SweetPad</a></strong> extension enables developers to use VS Code or Cursor for the entire Swift application development lifecycle on Apple platforms. It eliminates the need to constantly switch to Xcode by integrating essential tools such as xcodebuild, xcode-build-server, and swift-format. Developers can build, run and debug Swift applications for iOS, macOS and watchOS directly from their IDEs, while also managing simulators and deploying to devices without opening Xcode.</p>"
Tape/Z Tools for Assembly Program Exploration for Z/OS,Assess,Tools,TRUE,new,"<p><strong><a href=""https://github.com/avishek-sen-gupta/tape-z"">Tape/Z (Tools for Assembly Program Exploration for Z/OS)</a></strong> is an evolving toolkit for analyzing mainframe HLASM (High Level Assembler) code. Developed by a Thoughtworker, it provides capabilities such as parsing, control flow graph construction, dependency tracing and flowchart visualization. We’ve long noted the scarcity of open, community-driven tools in the mainframe space, where most options remain proprietary or tied to vendor ecosystems. Tape/Z helps close that gap by offering accessible, scriptable analysis capabilities. Alongside <a href=""https://github.com/avishek-sen-gupta/cobol-rekt"">COBOL REKT</a> — a companion toolkit for COBOL that we’ve also used multiple times with clients — it represents encouraging progress toward modern, developer-friendly tooling for mainframe systems.</p>"
Fastify,Adopt,languages-and-frameworks,FALSE,moved in,"<p>We continue to have a positive experience with <strong><a href=""https://www.fastify.io/"">Fastify</a></strong> — a fast, unopinionated, low-overhead web framework for <a href=""/radar/platforms/node-js"">Node.js</a>. It provides all the essential capabilities of a minimal web framework — including parsing, validation and serialization — along with a robust plugin system and strong community support. Our teams have seen no significant downsides in using Fastify over alternatives such as <a href=""https://expressjs.com/"">Express.js</a>, while also gaining measurable performance improvements, making it a compelling choice for minimal web development in Node.js.</p>"
LangGraph,Adopt,languages-and-frameworks,FALSE,moved in,"<p><strong><a href=""https://github.com/langchain-ai/langgraph"">LangGraph</a></strong> is an orchestration framework for building stateful <a href=""https://www.thoughtworks.com/radar/techniques/llm-powered-autonomous-agents"">multi-agent applications</a> using LLMs. It provides low-level primitives such as nodes and edges, along with built-in features that give developers granular control over agent workflows, memory management and state persistence. This means developers can start with a simple pre-built graph and scale to complex, evolving agent architectures. With support for streaming, advanced context management and resilience patterns like model fallbacks and tool error handling, LangGraph enables you to build robust, production-grade agentic applications. Its graph-based approach ensures predictable, customizable workflows and simplifies debugging and scaling. Our teams have had strong results using LangGraph to build multi-agent systems thanks to its lightweight and modular design.</p>"
vLLM,Adopt,languages-and-frameworks,FALSE,moved in,"<p><strong><a href=""https://github.com/vllm-project/vllm"">vLLM</a></strong> is a high-throughput, memory-efficient inference engine for LLMs that can run in the cloud or on-premises. It supports multiple <a href=""https://docs.vllm.ai/en/latest/models/supported_models.html"">model architectures</a> and popular open-source models. Our teams deploy dockerized vLLM workers on GPU platforms such as NVIDIA DGX and Intel HPC, hosting models including <a href=""https://huggingface.co/blog/llama31"">Llama 3.1 (8B and 70B)</a>, <a href=""https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"">Mistral 7B</a> and <a href=""https://huggingface.co/defog/llama-3-sqlcoder-8b"">Llama-SQL</a> for developer coding assistance, knowledge search and natural language database interactions. vLLM is compatible with the OpenAI SDK standard, enabling consistent model serving. Azure's <a href=""https://ai.azure.com/explore/models"">AI Model Catalog</a> uses a custom inference container built on vLLM to boost serving performance, with vLLM as the default inference engine due to its high throughput and efficient memory management. The vLLM framework has become a go-to framework for large-scale model deployments.</p>"
Crossplane,Trial,languages-and-frameworks,FALSE,moved in,"<p>Since its last appearance on the Radar, <strong><a href=""https://www.crossplane.io"">Crossplane</a></strong> adoption has continued to grow, particularly for extending Kubernetes clusters. In our work, we've found Crossplane excels in specific use cases rather than as a general-purpose infrastructure-as-code (IaC) tool. Our earlier observations still hold true: Crossplane works best as a companion to workloads deployed within Kubernetes, not as a full replacement for tools like Terraform. Teams that went ""all-in"" on Crossplane as their primary IaC solution often struggled, whereas those using it pragmatically — for targeted, custom use-cases — saw strong results. Offloading resource lifecycle management to Crossplane while using XRD APIs for lightweight customization has proven especially effective. Crossplane is particularly valuable when managing resources whose lifecycles are straightforward but not native to Kubernetes. While it can now create Kubernetes clusters — a capability that was previously missing — we advise caution when adopting Crossplane as a complete Terraform replacement. In our experience, it works best when IaC serves as the foundational layer, with Crossplane layered on top for specialized requirements.</p>"
deepeval,Trial,languages-and-frameworks,FALSE,moved in,"<p><strong><a href=""https://github.com/confident-ai/deepeval"">DeepEval</a></strong> is an open-source, Python-based evaluation framework for assessing LLM performance. It can be used to evaluate <a href=""/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> and other applications built with frameworks such as <a href=""/radar/languages-and-frameworks/llamaindex"">LlamaIndex</a> or <a href=""/radar/languages-and-frameworks/langchain"">LangChain</a>, as well as to baseline and benchmark models. DeepEval goes beyond word-matching scores, assessing accuracy, relevance and consistency to provide more reliable evaluation in real-world scenarios. It includes metrics such as hallucination detection, answer relevancy and hyperparameter optimization and supports <a href=""https://github.com/confident-ai/deepeval?tab=readme-ov-file#geval"">GEval</a> for creating custom, use case–specific metrics. Our teams are using DeepEval to fine-tune agentic outputs using the <a href=""/radar/techniques/llm-as-a-judge"">LLM as a judge</a> technique. It integrates with pytest and CI/CD pipelines, making it easy to adopt and valuable for continuous evaluation. For teams building LLM-based applications in regulated environments, <a href=""https://github.com/UKGovernmentBEIS/inspect_ai"">Inspect AI</a>, developed by the UK AI Safety Institute, offers an alternative with stronger focus on auditing and compliance.</p>"
fastMCP,Trial,languages-and-frameworks,TRUE,new,"<p>The <a href=""/radar/platforms/model-context-protocol-mcp"">Model Context Protocol (MCP)</a> is rapidly becoming a standard for providing context and tooling for LLM applications. However, implementing an MCP server typically involves substantial boilerplate for setup, protocol handling and error management. <strong><a href=""https://gofastmcp.com/"">FastMCP</a></strong> is a Python framework that simplifies this process by abstracting away protocol complexity and allowing developers to define MCP resources and tooling through intuitive Python decorators. This abstraction enables teams to focus on business logic, resulting in cleaner, more maintainable MCP implementations.</p>
<p>While FastMCP 1.0 is already incorporated into the <a href=""https://github.com/modelcontextprotocol/python-sdk"">official SDK</a>, the MCP standard continues to evolve quickly.  We recommend monitoring the 2.0 release and ensuring teams stay aligned with changes to the official specification.</p>"
LiteLLM,Trial,languages-and-frameworks,FALSE,no change,"<p><strong><a href=""https://github.com/BerriAI/litellm?utm_source=chatgpt.com"">LiteLLM</a></strong> is a SDK that provides seamless integration with multiple LLM providers through a standardized <a href=""https://platform.openai.com/docs/guides/text-generation/chat-completions-api"">OpenAI API format</a>. It supports a wide range of <a href=""https://docs.litellm.ai/docs/providers"">providers and models</a>, offering a unified interface for text completion, embeddings and image generation. By abstracting provider-specific API differences, LiteLLM simplifies integration and automatically routes requests to the correct model endpoint. It also includes production-grade features such as guardrails, caching, logging, rate limiting and load balancing through its <a href=""https://docs.litellm.ai/docs/simple_proxy?utm_source=chatgpt.com"">proxy framework</a>. As organizations embed AI-powered applications more deeply into workflows, governance and observability become essential. Our teams have been using LiteLLM as an <a href=""https://www.cloudflare.com/en-gb/developer-platform/products/ai-gateway/"">AI gateway</a> to standardize, secure and gain visibility into enterprise-wide AI usage.</p>"
MLForecast,Trial,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/Nixtla/mlforecast"">MLForecast</a></strong> is a Python framework and library for time series forecasting that applies machine learning models to large-scale data sets. It simplifies the typically complex process of automated feature engineering — including lags, rolling statistics and date-based features — and is one of the few libraries with native support for distributed computing frameworks like Spark and Dask, ensuring scalability. It also supports probabilistic forecasting using methods such as conformal prediction, providing quantitative measures of forecast uncertainty. In our evaluation, MLForecast scaled efficiently to millions of data points and consistently outperformed comparable tools. For teams looking to rapidly operationalize time series forecasting on high-volume data, MLForecasty is a compelling choice.</p>"
Nuxt,Trial,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://nuxt.com/"">Nuxt</a></strong> is an opinionated meta-framework built on top of <a href=""/radar/languages-and-frameworks/vue-js"">Vue.js</a> for creating full-stack web applications, often known as the ""<a href=""/radar/languages-and-frameworks/next-js"">Next.js</a> for Vue.js."" Similar to its React counterpart, Nuxt provides SEO-friendly capabilities such as pre-rendering, server-side rendering (SSR) and metadata management, making it a strong choice for building performance-oriented, SEO-optimized websites on the Vue.js stack.</p>
<p>Nuxt is backed by Vercel, the same company behind Next.js, and supported by a strong community and ecosystem of official and third-party modules. These modules simplify integration of features such as image processing, sitemaps and Tailwind CSS. Nuxt is a good choice for teams looking for a comprehensive, opinionated framework for building SEO-friendly applications with Vue.js.</p>"
Phoenix,Trial,languages-and-frameworks,FALSE,no change,"<p>We continue to have positive experiences with <strong><a href=""https://www.phoenixframework.org/"">Phoenix</a></strong>, a server-side web MVC framework written in <a href=""/radar/languages-and-frameworks/elixir"">Elixir</a>. Phoenix builds on the rapid application development and developer experience learnings of Ruby on Rails, while also advancing into functional programming paradigms.</p>
<p>In this volume, we’re highlighting the release of <a href=""https://phoenixframework.org/blog/phoenix-liveview-1.0-released"">Phoenix LiveView 1.0</a>. LiveView is an HTML-over-the-wire solution — similar to <a href=""/radar/languages-and-frameworks/htmx"">HTMX</a> or <a href=""/radar/techniques/hotwire"">Hotwire</a> — that enables developers to build rich, real-time user experiences entirely with server-rendered HTML. While similar technologies usually handle partial updates through HTML switching, LiveView provides a full component-based architecture via <a href=""https://hexdocs.pm/phoenix_live_view/Phoenix.LiveComponent.html"">LiveComponents</a>, offering composition, props passing, state management and lifecycle hooks akin to <a href=""/radar/languages-and-frameworks/react-js"">React</a> or <a href=""/radar/languages-and-frameworks/vue-js"">Vue.js</a>. LiveView combines the productivity and scalability of Phoenix with robust complexity management, making it well-suited for building highly interactive frontends without the need for a full JavaScript framework.</p>"
Presidio,Trial,languages-and-frameworks,FALSE,moved in,"<p><strong><a href=""https://github.com/microsoft/presidio"">Presidio</a></strong> is a data protection SDK for <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/#analyze-text-for-pii-entities"">identifying</a> and <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/#anonymize-text-with-identified-pii-entities"">anonymizing</a> sensitive data in structured and unstructured text. It detects personally identifiable information (PII) such as credit card numbers, names and locations using named entity recognition, regular expressions and rule-based logic. Presidio supports <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/#create-custom-pii-entity-recognizers"">custom</a> entity recognizers and de-identification pipelines, allowing organizations to tailor it to their privacy and compliance needs. Our teams have used Presidio in enterprise environments with strict data-sharing controls when integrating with LLMs. While it automates the detection of sensitive information, it isn’t foolproof and may miss or misidentify entities. Teams should exercise caution when relying on its results.</p>"
PydanticAI,Trial,languages-and-frameworks,FALSE,moved in,"<p><a href=""https://pydantic.dev/articles/pydantic-ai-v1"">Pydantic AI</a> continues to prove itself as a stable, well-supported, open-source framework for building GenAI agents in production. Built on the trusted <a href=""/radar/languages-and-frameworks/pydantic"">Pydantic</a> foundation, it offers strong type safety, first-class observability through <a href=""/radar/languages-and-frameworks/opentelemetry"">OpenTelemetry</a> and built-in evaluation tooling. The release of version 1.0  on September 4, 2025 marked a significant milestone in its maturity. Since then, we’ve found it reliable and widely adopted for its simplicity and maintainability, joining the ranks of other popular agent frameworks like [LangChain]/radar/languages-and-frameworks/langchain) and <a href=""/radar/languages-and-frameworks/langgraph"">LangGraph</a>.</p>
<p>Recent updates have made it easier to implement Model Context Protocol (MCP) servers and clients, with added support for emerging standards such as AG-UI and A2A. With its clean API and growing ecosystem, Pydantic AI has become a compelling choice for our teams building production-ready GenAI applications in Python.</p>"
Tauri,Trial,languages-and-frameworks,FALSE,moved in,"<p><strong><a href=""https://github.com/tauri-apps/tauri"">Tauri</a></strong> is a framework for building high-performance desktop applications using a single web UI codebase. Unlike traditional web wrappers such as <a href=""https://www.electronjs.org"">Electron</a>, Tauri is built on <a href=""/radar/languages-and-frameworks/rust"">Rust</a> and leverages the operating system's native webview, resulting in smaller binaries and stronger security. We first evaluated Tauri several years ago; since then, it has expanded beyond desktop to support iOS and Android. The latest version introduces a more flexible permission and scope model, replaces the older permission list and features a hardened inter-process communication (IPC) layer that supports raw data transfer and improves performance. These updates are backed by an external security audit. Together with official distribution guidelines for major app stores, these improvements further strengthen Tauri's position in the cross-platform development space.</p>"
Agent Development Kit (ADK),Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://google.github.io/adk-docs/"">Agent Development Kit</a> (ADK)</strong> is a framework for developing and deploying AI agents that applies modern software engineering discipline rather than relying solely on prompting. It introduces familiar abstractions such as classes, methods, workflow patterns and CLI support. Compared to frameworks like <a href=""/radar/languages-and-frameworks/langgraph"">LangGraph</a> or <a href=""/radar/languages-and-frameworks/crewai"">CrewAI</a>, ADK’s strength lies in its deep integration with Google’s AI infrastructure, providing enterprise-ready grounding, data access and monitoring. It’s also designed for interoperability, supporting tool wrappers and the <a href=""/radar/platforms/agent-to-agent-a2a-protocol"">A2A protocol</a> for agent-to-agent communication. For organizations already invested in GCP, ADK presents a promising foundation for building scalable, secure and manageable agentic architecture. Though still early in its evolution, it signals Google’s direction toward a native, full-stack agent development environment. We recommend keeping a close eye on its maturity and ecosystem growth.</p>"
Agno,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/agno-agi/agno"">Agno</a></strong> is a framework for building, running and managing multi-agent systems. It offers the flexibility to create fully autonomous agents or controlled, step-based workflows, with built-in support for human-in-the-loop, session management, memory and knowledge. We appreciate its focus on efficiency, with impressive agent startup times and low memory consumption. Agno also comes with its runtime, <a href=""https://docs.agno.com/agent-os/introduction"">AgentOS</a>, a FastAPI application with an integrated control plane for streamlined testing, monitoring and management of agentic systems.</p>"
assistant-ui,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/assistant-ui/assistant-ui"">assistant-ui</a></strong> is an open-source TypeScript and React library for AI chat interfaces. It handles the complex parts of chat UI implementation — such as streaming, state management for message editing and branch switching and common UX features — while allowing developers to design their own components using Radix primitives. It supports integration with popular runtimes, including <a href=""/radar/languages-and-frameworks/vercel-ai-sdk"">Vercel AI SDK</a> and <a href=""/radar/languages-and-frameworks/langgraph"">LangGraph</a>, and offers customizable runtime solutions for complex use cases. We’ve successfully built a simple chat interface with assistant-ui and have been pleased with the results.</p>"
AutoRound,Assess,languages-and-frameworks,TRUE,new,"<p>Intel's <strong><a href=""https://github.com/intel/auto-round"">AutoRound</a></strong> is an advanced quantization algorithm for compressing large AI models, such as LLMs and vision language models (VLMs), with minimal loss of accuracy. It reduces model size to ultra-low bit widths (2–4 bits) using sign-gradient descent optimization and applies mixed bit widths across layers for optimal efficiency. This quantization process is also remarkably fast: You can quantize a 7-billion-parameter model in just minutes on a single GPU. Since AutoRound integrates with popular inference engines such as <a href=""/radar/languages-and-frameworks/vllm"">vLLM</a> and <a href=""https://github.com/huggingface/transformers"">Transformers</a>, it's an attractive option for quantizing models.</p>"
Browser Use,Assess,languages-and-frameworks,FALSE,no change,"<p><strong><a href=""https://github.com/browser-use/browser-use"">Browser Use</a></strong> is an open-source Python library that enables LLM-based agents to operate web browsers and interact with web applications. It can navigate, enter data, extract text and manage multiple tabs to coordinate actions across applications. The library is particularly useful when AI agents need to access, manipulate or retrieve information from web content. It supports a range of LLMs and leverages <a href=""/radar/languages-and-frameworks/playwright"">Playwright</a> to combine visual understanding with HTML structure extraction for richer web interactions. Our teams integrated Browser Use with the Pytest framework and <a href=""https://github.com/allure-framework/allure2"">Allure</a> reporting to explore automated testing with LLMs. Test steps were written in natural language for the agent to execute, capturing screenshots on assertions or failures. The goal was to enable off-hours QA by automatically pulling test cases from Confluence for post-development verification. Early results are promising, though agent’s post-task responses often lack detailed failure descriptions, so require custom error reporting.</p>"
DeepSpeed,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/deepspeedai/DeepSpeed"">DeepSpeed</a></strong> is a Python library that optimizes distributed deep learning for both training and inference. For training, it integrates technologies such as the Zero Redundancy Optimizer (ZeRO) and 3D parallelism to efficiently scale models across thousands of GPUs. For inference, it combines tensor, pipeline, expert and ZeRO parallelism with custom kernels and communication optimizations to minimize latency. DeepSpeed has powered some of the world's largest language models, including Megatron-Turing NLG (530B) and BLOOM (176B). It supports both dense and sparse models, delivers high system throughput and allows training or inference across multiple resource-constrained GPUs. The library integrates seamlessly with popular Hugging Face Transformers, PyTorch Lightning and Accelerate, making it a highly effective option for large-scale or resource-limited deep learning workloads.</p>"
Drizzle,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://orm.drizzle.team/"">Drizzle</a></strong> is a lightweight TypeScript ORM. Unlike <a href=""/radar/anguages-and-frameworks/prisma-orm"">Prisma ORM</a>, it gives developers both a simple SQL-like API and a more traditional ORM-style query interface. It also supports <a href=""https://orm.drizzle.team/docs/drizzle-kit-pull#drizzle-kit-pull"">extracting schemas from existing databases</a>, enabling both database-first and code-first approaches. Drizzle was designed with serverless environments in mind: It has a small bundle size and supports <a href=""https://orm.drizzle.team/docs/perf-queries"">prepared statements</a>, allowing SQL queries to be precompiled so that the database driver executes binary SQL directly instead of parsing queries each time. Its simplicity and serverless support make Drizzle an appealing choice for ORM use in the TypeScript ecosystem.</p>"
Java post-quantum cryptography,Assess,languages-and-frameworks,FALSE,stay,"<p>Quantum compute continues to advance rapidly, with SaaS offerings like <a href=""https://aws.amazon.com/braket/"">AWS Braket</a> now providing access to quantum algorithms across multiple architectures.</p>
<p>Since March, <a href=""https://openjdk.org/projects/jdk/24/"">Java 24</a> has introduced <strong>Java post-quantum cryptography</strong>, adding support for post-quantum cryptographic algorithms such as <a href=""https://csrc.nist.gov/pubs/fips/203/final"">ML-KEM</a> and <a href=""https://csrc.nist.gov/pubs/fips/204/final"">ML-DSA</a>, and <a href=""https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotnet-10/overview"">.Net 10</a> has expanded its support as well. Our advice is simple: if you’re building software in these languages, begin adopting quantum-safe algorithms now to future-proof your systems.</p>"
kagent,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://kagent.dev/"">Kagent</a></strong> is an open-source framework for running agentic AI inside Kubernetes clusters. It enables LLM-based agents to plan and execute operational tasks such as diagnosing issues, remediating configurations or interacting with observability tools through Kubernetes-native APIs and Model Context Protocol (MCP) integrations. Its goal is to bring ""AgentOps"" to cloud-native infrastructure by combining declarative management with autonomous reasoning.</p>
<p>As a CNCF Sandbox project, Kagent should be introduced with care, particularly given the risks of granting LLMs operational management capabilities. Techniques such as <a href=""/radar/techniques/toxic-flow-analysis-for-ai"">toxic flow analysis</a> can be especially valuable when assessing and mitigating these risks.</p>"
Lang Extract,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/google/langextract"">LangExtract</a></strong> is a Python library that uses LLMs to extract structured information from unstructured text based on user-defined instructions. It processes domain-specific materials — such as clinical notes and reports — identifying and organizing key details while keeping each extracted data point traceable to its source. The extracted entities can be exported as a <code>.jsonl</code> file, a standard format for language model data and visualized through an interactive HTML interface for contextual review. Our teams evaluated LangExtract for extracting entities to populate a domain knowledge graph and found it effective for transforming complex documents into structured, machine-readable representations.</p>"
Langflow,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/langflow-ai/langflow"">LangFlow</a></strong> is an open-source, low-code platform for building and visualizing LLM workflows. Built on top of LangChain, it allows developers to chain prompts, tools, vector databases and memory components through a drag-and-drop interface, while still supporting custom Python code for advanced logic. It's particularly useful for prototyping agentic applications without writing full back-end code. However, LangFlow is still relatively new and has some rough edges for production use. Our usual caution around <a href=""/radar/platforms/low-code-platforms"">low-code platforms</a> applies here. That said, we like that LangFlow's workflows can be defined and versioned as code, which can mitigate some of the drawbacks of low-code platforms.</p>"
LMCache,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/LMCache/LMCache"">LMCache</a></strong> is a key-value (KV) cache solution that accelerates LLM serving infrastructure. It acts as a specialized caching layer across a pool of LLM inference engines, storing precomputed KV cache entries for texts likely to be processed multiple times, such as chat histories or document collections. By persisting these values on disk, prefill computations can be offloaded from the GPU, reducing time-to-first-token (TTFT) and cutting inference costs across demanding workloads like RAG pipelines, multi-turn chat applications and agentic systems. You can integrate LMCache with major inference servers such as <a href=""https://docs.vllm.ai/en/latest/examples/others/lmcache.html"">vLLM</a> or <a href=""https://docs.nvidia.com/dynamo/latest/components/backends/vllm/LMCache_Integration.html"">NVIDIA Dynamo</a> and think it’s worth assessing its impact on your setup.</p>"
Mem0,Assess,languages-and-frameworks,TRUE,new,"<p><a href=""https://mem0.ai/""><strong>Mem0</strong></a> is a memory layer designed for AI agents. Naive approaches often store entire chat histories in a database and reuse them in future conversations, which leads to excessive token usage. Mem0 replaces this with a more sophisticated architecture that separates memory into short-term recall and an intelligent long-term layer that extracts and stores only salient facts and relationships. Its architecture combines a vector store for semantic similarity with a knowledge graph for understanding temporal and relational data. This design significantly reduces context token usage while enabling agents to maintain long-term awareness, which is extremely useful for personalization and many other use cases</p>"
Open Security Control Assessment Language (OSCAL),Assess,languages-and-frameworks,TRUE,new,"<p>The <strong><a href=""https://pages.nist.gov/OSCAL/"">Open Security Controls Assessment Language</a> (OSCAL)</strong> is an open, machine-readable information exchange format designed to increase automation in compliance and risk management, and help teams move away from text-based manual approaches. Led by the National Institute of Standards and Technology (NIST), OSCAL provides <a href=""https://pages.nist.gov/OSCAL/learn/concepts/"">standard representations in XML, JSON and YAML</a> for expressing security controls associated with industry frameworks such as SOC 2 and PCI, as well as government frameworks such as <a href=""https://github.com/GSA/fedramp-automation"">FedRAMP</a> in the United States, Singapore's <a href=""https://info.standards.tech.gov.sg/control-catalog/"">Cybersecurity Control Catalogue</a> and Australia's <a href=""https://www.cyber.gov.au/business-government/asds-cyber-security-frameworks/ism/ism-oscal-releases"">Information Security Manual</a>.</p>
<p>While OSCAL has not yet been widely adopted outside the public sector and its<a href=""https://github.com/oscal-club/awesome-oscal""> ecosystem</a> is still maturing, we’re excited by its potential to streamline security assessments, reduce reliance on spreadsheets and box-ticking exercises and even enable automated compliance when incorporated into compliance-as-code and <a href=""/radar/techniques/continuous-compliance"">continuous compliance</a> platforms.</p>"
OpenInference,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://github.com/Arize-ai/openinference"">OpenInference</a></strong> is a set of conventions and plugins; it’s complementary to <a href=""/radar/languages-and-frameworks/opentelemetry"">OpenTelemetry</a> and designed to observe AI applications. It provides standardized instrumentation for machine-learning frameworks and <a href=""https://github.com/Arize-ai/openinference/tree/main?tab=readme-ov-file#libraries"">libraries</a>, which helps developers trace LLM invocations along with surrounding context such as vector store retrievals or external tool calls to APIs and search engines. Spans can be exported to any OTEL-compatible collector, ensuring alignment with existing telemetry pipelines. We previously blipped <a href=""/radar/platforms/langfuse"">Langfuse</a>, a commonly used LLM observability platform — the <a href=""https://langfuse.com/guides/cookbook/otel_integration_arize"">OpenInference SDK</a> can log traces into Langfuse and other OpenTelemetry-compatible observability platforms.</p>"
Valibot,Assess,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://valibot.dev/"">Valibot</a></strong> is a schema validation library in TypeScript. Like other popular TypeScript validation libraries such as <a href=""https://zod.dev/"">Zod</a> and <a href=""/radar/languages-and-frameworks/ajv"">Ajv</a>, it provides type inference, but its modular design sets it apart. This architecture allows bundlers to perform effective tree shaking and code splitting, including only the validation functions actually used. Valibot can reduce the bundle size by up to 95% compared to Zod in optimal scenarios. It’s an appealing choice for schema validation in environments where bundle size is critical, such as client-side validation or serverless functions.</p>"
vercel AI SDK,Assess,languages-and-frameworks,TRUE,new,"<p><a href=""https://ai-sdk.dev/docs/introduction"">Vercel AI SDK</a> is an open-source, full-stack toolkit for building AI-powered applications and agents in the TypeScript ecosystem. It consists of two main components: AI SDK Core standardizes model-agnostic LLM calls, supporting text generation, structured object generation and tool calling; AI SDK UI simplifies front-end development with streaming, state management and real-time UI updates in React, Vue, Next.js and Svelte, similar to <a href=""/radar/languages-and-frameworks/assistant-ui"">assistant-ui</a>. For teams already working within the TypeScript and Next.js ecosystem, Vercel AI SDK provides a fast, seamless way to build AI applications with rich, client-side experiences.</p>"
